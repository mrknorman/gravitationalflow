{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Composing Datasets\n",
    "\n",
    "In this notebook we will examine how we can use the elements we have encountered so far, in order to construct a TensorFlow dataset which will allow us to train machine learning models with data generated in real time. This is the core and most usefull functionality of the dataset portion of GravyFlow.\n",
    "\n",
    "As usual, we begin by performing the relevent imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in imports\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "# Dependency imports: \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Flatten, Dropout, ELU\n",
    "from tensorflow.keras.models import Model\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "# Import the GravyFlow module.\n",
    "import gravyflow as gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gf.env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_features(features, labels):\n",
    "    labels['INJECTION_MASKS'] = labels['INJECTION_MASKS'][0]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gabbard_model(\n",
    "        input_shape_onsource : int, \n",
    "        input_shape_offsource : int\n",
    "    ) -> tf.keras.Model:\n",
    "    \n",
    "    # Define the inputs based on the dictionary keys and expected shapes\n",
    "    # Replace `input_shape_onsource` and `input_shape_offsource` with the actual shapes\n",
    "    onsource_input = Input(shape=input_shape_onsource, name=\"ONSOURCE\")\n",
    "    offsource_input = Input(shape=input_shape_offsource, name=\"OFFSOURCE\")\n",
    "\n",
    "    # Pass the inputs to your custom Whiten layer\n",
    "    # Assuming your Whiten layer can handle multiple inputs\n",
    "    whiten_output = gf.Whiten()([onsource_input, offsource_input])\n",
    "\n",
    "    x = gf.Reshape()(whiten_output)\n",
    "    \n",
    "    # Convolutional and Pooling layers\n",
    "    x = Conv1D(8, 64, padding='same', name=\"Convolutional_1\")(x)\n",
    "    x = ELU(name=\"ELU_1\")(x)\n",
    "    x = MaxPooling1D(pool_size=4, strides=4, name=\"Pooling_1\", padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(8, 32, padding='same', name=\"Convolutional_2\")(x)\n",
    "    x = ELU(name=\"ELU_2\")(x)\n",
    "    x = Conv1D(16, 32, padding='same', name=\"Convolutional_3\")(x)\n",
    "    x = ELU(name=\"ELU_3\")(x)\n",
    "    x = MaxPooling1D(pool_size=4, strides=4, name=\"Pooling_3\", padding=\"same\")(x)\n",
    "\n",
    "    x = Conv1D(16, 16, padding='same', name=\"Convolutional_4\")(x)\n",
    "    x = ELU(name=\"ELU_4\")(x)\n",
    "    x = Conv1D(32, 16, padding='same', name=\"Convolutional_5\")(x)\n",
    "    x = ELU(name=\"ELU_5\")(x)\n",
    "    x = MaxPooling1D(pool_size=4, strides=4, name=\"Pooling_5\", padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(32, 16, padding='same', name=\"Convolutional_6\")(x)\n",
    "    x = ELU(name=\"ELU_6\")(x)\n",
    "    \n",
    "    # Flatten layer\n",
    "    x = Flatten(name=\"Flatten\")(x)\n",
    "    \n",
    "    # Dense layers with dropout\n",
    "    x = Dense(64, name=\"Dense_1\")(x)\n",
    "    x = ELU(name=\"ELU_7\")(x)\n",
    "    x = Dropout(0.5, name=\"Dropout_1\")(x)\n",
    "    \n",
    "    x = Dense(64, name=\"Dense_2\")(x)\n",
    "    x = ELU(name=\"ELU_8\")(x)\n",
    "    x = Dropout(0.5, name=\"Dropout_2\")(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid', name=\"INJECTION_MASKS\")(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[onsource_input, offsource_input], outputs=outputs, name=\"custom_model\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with env:\n",
    "    # This object will be used to obtain real interferometer data based on specified parameters.\n",
    "    ifo_data_obtainer : gf.IFODataObtainer = gf.IFODataObtainer(\n",
    "        observing_runs=gf.ObservingRun.O3, # Specify the observing run (e.g., O3).\n",
    "        data_quality=gf.DataQuality.BEST,  # Choose the quality of the data (e.g., BEST).\n",
    "        data_labels=[                      # Define the types of data to include.\n",
    "            gf.DataLabel.NOISE, \n",
    "            gf.DataLabel.GLITCHES\n",
    "        ],\n",
    "        segment_order=gf.SegmentOrder.RANDOM, # Order of segment retrieval (e.g., RANDOM).\n",
    "        force_acquisition=True,               # Force the acquisition of new data.\n",
    "        cache_segments=False                  # Choose not to cache the segments.\n",
    "    )\n",
    "\n",
    "    # Initialize the noise generator wrapper:\n",
    "    # This wrapper will use the ifo_data_obtainer to generate real noise based on the specified parameters.\n",
    "    noise: gf.NoiseObtainer = gf.NoiseObtainer(\n",
    "        ifo_data_obtainer=ifo_data_obtainer, # Use the previously set up IFODataObtainer object.\n",
    "        noise_type=gf.NoiseType.REAL,        # Specify the type of noise as REAL.\n",
    "        ifos=gf.IFO.L1                       # Specify the interferometer (e.g., LIGO Livingston L1).\n",
    "    )\n",
    "\n",
    "    scaling_method : gf.ScalingMethod = gf.ScalingMethod(\n",
    "        value=gf.Distribution(\n",
    "            min_=8.0,\n",
    "            max_=15.0,\n",
    "            type_=gf.DistributionType.UNIFORM\n",
    "        ),\n",
    "        type_=gf.ScalingTypes.SNR\n",
    "    )\n",
    "\n",
    "    # Define a uniform distribution for the mass of the first object in solar masses.\n",
    "    mass_1_distribution_msun : gf.Distribution = gf.Distribution(\n",
    "        min_=10.0, \n",
    "        max_=60.0, \n",
    "        type_=gf.DistributionType.UNIFORM\n",
    "    )\n",
    "\n",
    "    # Define a uniform distribution for the mass of the second object in solar masses.\n",
    "    mass_2_distribution_msun : gf.Distribution = gf.Distribution(\n",
    "        min_=10.0, \n",
    "        max_=60.0, \n",
    "        type_=gf.DistributionType.UNIFORM\n",
    "    )\n",
    "\n",
    "    # Define a uniform distribution for the inclination of the binary system in radians.\n",
    "    inclination_distribution_radians : gf.Distribution = gf.Distribution(\n",
    "        min_=0.0, \n",
    "        max_=np.pi, \n",
    "        type_=gf.DistributionType.UNIFORM\n",
    "    )\n",
    "\n",
    "    # Initialize a PhenomD waveform generator with the defined distributions.\n",
    "    # This generator will produce waveforms with randomly varied masses and inclination angles.\n",
    "    phenom_d_generator : gf.WaveformGenerator = gf.cuPhenomDGenerator(\n",
    "        mass_1_msun=mass_1_distribution_msun,\n",
    "        mass_2_msun=mass_2_distribution_msun,\n",
    "        inclination_radians=inclination_distribution_radians,\n",
    "        scaling_method=scaling_method,\n",
    "        injection_chance=0.5 # Set so half produced examples will not contain this signal\n",
    "    )\n",
    "    \n",
    "    dataset : tf.data.Dataset = gf.Dataset(       \n",
    "        noise_obtainer=noise,\n",
    "        waveform_generators=phenom_d_generator,\n",
    "        input_variables=[\n",
    "            gf.ReturnVariables.ONSOURCE, \n",
    "            gf.ReturnVariables.OFFSOURCE, \n",
    "        ],\n",
    "        output_variables=[\n",
    "            gf.ReturnVariables.INJECTION_MASKS\n",
    "        ]\n",
    "    ).map(adjust_features)\n",
    "    \n",
    "    for input_example, _ in dataset.take(1):\n",
    "        input_shape_onsource = input_example[\"ONSOURCE\"].shape[1:]  # Exclude batch dimension    \n",
    "        input_shape_offsource = input_example[\"OFFSOURCE\"].shape[1:] \n",
    "\n",
    "    model = create_gabbard_model(input_shape_onsource, input_shape_offsource)\n",
    "\n",
    "    # Now you can print the model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Model compilation\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',  # Or any other loss function appropriate for your task\n",
    "        metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "examples_per_epoch : int = int(1E5)\n",
    "with env: \n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        epochs=10,  # Number of epochs to train for\n",
    "        steps_per_epoch= examples_per_epoch // gf.Defaults.num_examples_per_batch\n",
    "        #validation_data=validation_dataset  # Assuming you have a validation dataset\n",
    "        # Add other parameters as needed, such as callbacks\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('gravyflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0be68b4d857a3b55dcb84670c9ea8054a433650dc9da871ef592f2f480116ffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
