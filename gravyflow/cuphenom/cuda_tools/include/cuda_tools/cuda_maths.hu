#ifndef CUDA_MATHS_HU
#define CUDA_MATHS_HU

#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <complex.h> 

#include <iostream>

#include <cuda.h>
#include <cuda_runtime.h>
#include <curand.h>
#include <cufft.h>
#include <cuda_fp16.h>

#define TEST_LENGTH 10000000
#define BLOCK_SIZE 256
#define MAX_ERR 1e-5

#define CUDA_CALL(e) do { if((e)!=cudaSuccess) { \
    printf("Error: %s at %s:%d\n",cudaGetErrorString(e),__FILE__,__LINE__);\
    return EXIT_FAILURE;}} while(0)
#define CURAND_CALL(x) do { if((x)!=CURAND_STATUS_SUCCESS) { \
    printf("Error at %s:%d\n",__FILE__,__LINE__);\
    return EXIT_FAILURE;}} while(0)
#define CUFFT_CALL(x) do { if((x)!=CUFFT_SUCCESS) { \
    printf("Error at %s:%d\n",__FILE__,__LINE__);\
    return EXIT_FAILURE;}} while(0)

#define H_PI __floats2half2_rn((float)M_PI, (float)M_PI)

static const char *_cufftGetErrorEnum(cufftResult error) 
{
    switch (error) {
        case CUFFT_SUCCESS:
            return "CUFFT_SUCCESS";

        case CUFFT_INVALID_PLAN:
            return "CUFFT_INVALID_PLAN";

        case CUFFT_ALLOC_FAILED:
            return "CUFFT_ALLOC_FAILED";

        case CUFFT_INVALID_TYPE:
            return "CUFFT_INVALID_TYPE";

        case CUFFT_INVALID_VALUE:
            return "CUFFT_INVALID_VALUE";

        case CUFFT_INTERNAL_ERROR:
            return "CUFFT_INTERNAL_ERROR";

        case CUFFT_EXEC_FAILED:
            return "CUFFT_EXEC_FAILED";

        case CUFFT_SETUP_FAILED:
            return "CUFFT_SETUP_FAILED";

        case CUFFT_INVALID_SIZE:
            return "CUFFT_INVALID_SIZE";

        case CUFFT_UNALIGNED_DATA:
            return "CUFFT_UNALIGNED_DATA";

        case CUFFT_INCOMPLETE_PARAMETER_LIST:
            return "CUFFT_INCOMPLETE_PARAMETER_LIST";

        case CUFFT_INVALID_DEVICE:
            return "CUFFT_INVALID_DEVICE";

        case CUFFT_PARSE_ERROR:
            return "CUFFT_PARSE_ERROR";

        case CUFFT_NO_WORKSPACE:
            return "CUFFT_NO_WORKSPACE";

        case CUFFT_NOT_IMPLEMENTED:
            return "CUFFT_NOT_IMPLEMENTED";

        case CUFFT_LICENSE_ERROR:
            return "CUFFT_LICENSE_ERROR";

        case CUFFT_NOT_SUPPORTED:
            return "CUFFT_NOT_SUPPORTED";
            
        default:
            return "<unknown>";
    }

    return "<unknown>";
}

using namespace std;

typedef __half2 float16_2_t;

__device__ __forceinline__ float _square(
    const float number
    ) {
    return number*number;
}

__device__ __forceinline__ float _cube(
    const float number
    ) {
    return number*number*number;
}

__device__ __forceinline__ float _quart(
    const float number
    ) {
    const float pow2 = _square(number);
    return pow2 * pow2;
}

extern "C" void print16(char *name, float16_2_t value)
{	
	printf("%s: %f, %f \n", name, (float)value.x, (float)value.y);
}

__device__ __forceinline__ void __print16(char *name, float16_2_t value)
{	
	printf("%s: %f, %f \n", name, (float)value.x, (float)value.y);
}

__device__ __forceinline__ float _calcNextPow2(const float n)
{
   // Use pow here, not bit-wise shift, as the latter seems to run against an
   // upper cutoff long before SIZE_MAX, at least on some platforms:
   return powf(2.0f, ceilf(log2f(n)));
}


extern "C" __global__ void arrayToFloat16_2(
		  float       *array_1, 
		  float16_2_t *array_2,  
	const int32_t      num_elements
    ) {
    
    /**
     * Inplace addition of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be added to. 
     *           float   *array_2: array to add to array_1.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaAdd()
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, add array_2 element to array 1:
    if (tid < num_elements) 
	{    
		const float16_2_t value = __float2half2_rn(array_1[tid]);
        array_2[tid] = value;
    }
}

extern "C" __global__ void arrayFloat16_2ToFloat(
          float         *array_1, 
    const float16_2_t   *array_2,  
	const int32_t        num_elements
    ) {
    
    /**
     * Inplace addition of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be added to. 
     *           float   *array_2: array to add to array_1.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaAdd()
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, add array_2 element to array 1:
    if (tid < num_elements) 
	{    
        array_1[tid]                = (float)(array_2[tid].x);
		array_1[tid + num_elements] = (float)(array_2[tid].y);	
    }
}

__device__ __forceinline__ float16_2_t __h2pow(
	const __half2 base, 
	const __half2 exponent
	) {

	__half2 result = h2exp(exponent * h2log(base));
	
	if ((float)base.x == 0.0f)
    {
        if ((float)exponent.x == 0.0f)
        {
            // 0 to the power of 0 is undefined, return error
            result.x = NAN;
        }
        else
        {
            // 0 to the power of anything except 0 is 0
            result.x = (__half) 0.0;
        }
    } 
	else if ((float)base.x < 0.0f)
    {
        if ((float)exponent.x == floorf((float)exponent.x))
        {
            // Negative number to the power of an integer is valid
            result.x = 
				(__half) expf((float)exponent.x * logf(-base.x)) 
				* (__half)(1 - 2 * ((int32_t )exponent.x % 2));
        }
        else
        {
            // Negative number to the power of a non-integer is invalid
            result.x = NAN;
        }
    }
	
	if ((float)base.y == 0.0f)
    {
        if ((float)exponent.y == 0.0f)
        {
            // 0 to the power of 0 is undefined, return error
            result.y = NAN;
        }
        else
        {
            // 0 to the power of anything except 0 is 0
            result.y = (__half) 0.0;
        }
    } 
	else if ((float)base.y < 0.0f)
    {
        if ((float)exponent.y == floorf((float)exponent.y))
        {
            // Negative number to the power of an integer is valid
            result.y = 
				(__half) expf((float)exponent.y * logf(-base.y))
				* (__half)(1 - 2 * ((int32_t )exponent.y % 2));
        }
        else
        {
            // Negative number to the power of a non-integer is invalid
            result.y = NAN;
        }
    }

    return result;
}

extern "C" int32_t hostCudaMemInsert(
		  void    *array_1, 
		  void    *array_2,  
    const size_t   element_size,
	const int32_t  num_elements
    ) {
    
    /**
     * Copy one gpu to array to another
     * @param 
     *       float   *array_1: destination. 
     *       float   *array_2: source.
     * const int32_t num_elements: num elements in array_1. 
     * @see
     * @return int32_t: error_code
     */
     
    const size_t total_size = element_size*(size_t)num_elements;
    
    CUDA_CALL(
        cudaMemcpy(
            array_1, 
            array_2, 
            total_size, 
            cudaMemcpyHostToDevice)
    );
    
    return 0;
}

extern "C" int32_t hostCudaMemCpy(
		  float   *array_1, 
		  float   *array_2,  
	const int32_t  num_elements
    ) {
    
    /**
     * Copy one gpu to array to another
     * @param 
     *       float   *array_1: destination. 
     *       float   *array_2: source.
     * const int32_t num_elements: num elements in array_1. 
     * @see
     * @return int32_t: error_code
     */
    
    CUDA_CALL(
        cudaMemcpy(
            array_1, 
            array_2, 
            sizeof(float)*(size_t)num_elements, 
            cudaMemcpyDeviceToDevice)
    );
    
    return 0;
}

extern "C" int32_t cudaToDevice(
    const void     *array, 
    const size_t    element_size,
	const int32_t   num_elements,
          void    **ret_array_g
    ) {
    
    const size_t total_size = element_size*(size_t)num_elements;
    
    void *array_g = NULL;
    CUDA_CALL(
        cudaMalloc(
            &array_g, 
            total_size
        )
    );
    CUDA_CALL(
        cudaMemcpy(
            array_g, 
            array, 
            total_size, 
            cudaMemcpyHostToDevice
        )
    );
    
    *ret_array_g = array_g;
    
    return 0;
}

extern "C" int32_t cudaToHost(
    const void     *array_g, 
    const size_t    element_size,
	const int32_t   num_elements,
          void    **ret_array
    ) {
    
    const size_t total_size = element_size*(size_t)num_elements;
    
    void *array = (void*)malloc(total_size);
    CUDA_CALL(
        cudaMemcpy(
            array, 
            array_g, 
            total_size, 
            cudaMemcpyDeviceToHost
        )
    );
    
    *ret_array = array;
    
    return 0;
}

extern "C" int32_t cudaToHost16(
    const float16_2_t  *array_g, 
	const int32_t       num_elements,
          float       **ret_array
    ) {
    
	const size_t total_size = (size_t)num_elements * 2 * sizeof(float);
	float *cast_array_g = NULL;
    CUDA_CALL(
        cudaMalloc(
            (void**)&cast_array_g, 
            total_size
        )
    );
	
	const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
	arrayFloat16_2ToFloat<<<grid_size,BLOCK_SIZE>>>( 
		  cast_array_g,  
		  array_g,
		  num_elements
    );
	
	float *array = (float*)malloc(total_size);
	
    CUDA_CALL(
        cudaMemcpy(
            array, 
            cast_array_g, 
            total_size, 
            cudaMemcpyDeviceToHost
        )
    );
	
	cudaFree(cast_array_g);
    
    *ret_array = array;
    
    return 0;
}

extern "C" int32_t cudaPrintIntArray(
    const char     *title,
    const int32_t  *array_g,
    const int32_t   num_elements
    ) {   
    
    int32_t *array = NULL;
    cudaToHost(
        (void*)array_g, 
        sizeof(int32_t),
        num_elements,
        (void**)&array
    );
    
    printf("%s: [", title);
    for (int32_t index = 0; index < num_elements; index++)
    {
        printf("%i,", array[index]);
    
    }
    printf("]\n");
    
    free(array);
    
    return 0;
};

extern "C" int32_t cudaPrintArray(
    const char    *title,
    const float   *array_g,
    const int32_t  num_elements
    ) {
    
    float *array = NULL;
    cudaToHost(
        (void*)array_g, 
        sizeof(float),
        num_elements,
        (void**)&array
    );       
    
    printf("%s: [", title);
    for (int32_t index = 0; index < num_elements; index++)
    {
        printf("%i: %e, ", index, array[index]);
    
    }
    printf("]\n");
    
    free(array);
    
    return 0;
};

extern "C" int32_t cudaPrintArray64(
    const char    *title,
    const double   *array_g,
    const int32_t  num_elements
    ) {
    
    double *array = NULL;
    cudaToHost(
        (void*)array_g, 
        sizeof(double),
        num_elements,
        (void**)&array
    );       
    
    printf("%s: [", title);
    for (int32_t index = 0; index < num_elements; index++)
    {
        printf("%f,", array[index]);
    
    }
    printf("]\n");
    
    free(array);
    
    return 0;
};

extern "C" int32_t cudaPrintArray16(
    const char        *title,
    const float16_2_t *array_g,
    const int32_t      num_elements
    ) {
    
    float16_2_t *array = NULL;
    cudaToHost(
        (void*)array_g, 
        sizeof(float16_2_t),
        num_elements,
        (void**)&array
    );       
    
    printf("%s: [", title);
    for (int32_t index = 0; index < num_elements; index++)
    {
        printf("%f : %f,", (float)array[index].x, (float)array[index].y);
    
    }
    printf("]\n");
    
    free(array);
    
    return 0;
};

extern "C" void printTestResult(
    const int32_t  pass,
    const char    *test_name
    ) {
    
    /**
     * Prints result of test
     * @param 
     *     const bool       pass: test result.
     *     const char *test_name: test name.
     * @see
     * @return void
     */
    
    const char *result = pass ? "PASSED" : "FAILED";
    printf("%s: %s.\n\n", test_name, result); 
}

extern "C" __global__ void cudaAdd(
		  float   *array_1, 
		  float   *array_2,  
	const int32_t  num_elements
    ) {
    
    /**
     * Inplace addition of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be added to. 
     *           float   *array_2: array to add to array_1.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaAdd()
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, add array_2 element to array 1:
    if (tid < num_elements) 
	{    
        array_1[tid] += array_2[tid];
    }
}

__global__ void cudaAddValue(
              float   *array, 
              float    value, 
        const int32_t  num_elements
    ) {
    
    /**
     * Inplace addition of a float array by a value using cuda GPU 
     * library.
     * @param 
     *           float   *array: array to be added. 
     *           float    value: value to add array_1 by.
     *     const int32_t num_elements: num elements in array_1. 
     * @see 
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, add value toa array element:
    if (tid < num_elements) 
	{    
        array[tid] += value;
    }
}

extern "C" int32_t hostCudaAddValue(
              float   *array, 
              double   value, 
        const int32_t  num_elements
    ) {
    
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaAddValue<<<grid_size,BLOCK_SIZE>>>
        (array, (float)value, num_elements);
 
     return 0;
}

extern "C" int32_t testCudaAdd(
    const int32_t verbosity
    ) {
    
    /**
     * Test cudaAdd function.
     * @param 
     *     const int32_t verbosity: Set verbosity of test.
     * @see cudaAdd()
     * @return pass or error function.
     */
    
    float *array_1, *array_2;
    float *array_1_g, *array_2_g; 
    int32_t pass = true;
        
    // Allocate host memory:
    array_1 = (float*)malloc(sizeof(float) * TEST_LENGTH);
    array_2 = (float*)malloc(sizeof(float) * TEST_LENGTH);

    // Initialize host arrays:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        array_1[index] = 2.0f;
        array_2[index] = 1.0f;
    }

    // Allocate device memory:
    CUDA_CALL(cudaMalloc((void**)&array_1_g, sizeof(float) * TEST_LENGTH));
    CUDA_CALL(cudaMalloc((void**)&array_2_g, sizeof(float) * TEST_LENGTH));

    // Transfer data from host to device memory:
    CUDA_CALL(
        cudaMemcpy(
            array_1_g, 
            array_1, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyHostToDevice)
        );
    CUDA_CALL(
        cudaMemcpy(
            array_2_g, 
            array_2, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyHostToDevice)
        );

    // Executing kernel:
    const int32_t grid_size  = ((TEST_LENGTH + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaAdd<<<grid_size,BLOCK_SIZE>>>(array_1_g, array_2_g, TEST_LENGTH);
        
    // Transfer data back to host memory:
    CUDA_CALL(cudaMemcpy(
        array_1, 
        array_1_g, 
        sizeof(float) * TEST_LENGTH, 
        cudaMemcpyDeviceToHost)
    );

    // Verification:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        pass *= (fabs(array_1[index] - 3.0*array_2[index]) < MAX_ERR);
    }
    
    //Print result:
    if (verbosity >= 3) 
	{    
        printTestResult(pass, "cudaAdd");
    }
    
    // Deallocate device memory:
    cudaFree(array_1_g);
    cudaFree(array_2_g);

    // Deallocate host memory:
    free(array_1); 
    free(array_2); 
    
    //Return pass value:
    return !pass;
}

extern "C" __global__ void cudaSubtract(
              float   *array_1   , 
              float   *array_2   , 
        const int32_t  num_elements
    ) {
    
     /**
     * Inplace subtraction of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be subtracted from. 
     *           float   *array_2: array to subtract from array_1.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaSubtract()
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, subtract array_2 element from array 1:
    if (tid < num_elements) 
	{    
        array_1[tid] -= array_2[tid];
    }
}

extern "C" int32_t hostCudaSubtract(
          float   *array_1, 
          float   *array_2, 
    const int32_t  num_elements
    ) {
    
    const int32_t grid_size = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaSubtract<<<grid_size,BLOCK_SIZE>>>
        (array_1, array_2, num_elements);
 
     return 0;
}

extern "C" __global__ void cudaSubtractValue(
              float   *array, 
              float    value, 
        const int32_t  num_elements
    ) {
    
     /**
     * Inplace subtraction of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be subtracted from. 
     *           float   *array_2: array to subtract from array_1.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaSubtract()
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, subtract array_2 element from array 1:
    if (tid < num_elements) 
	{    
        array[tid] -= value;
    }
}

__global__ void cudaSubtractValue(
              float16_2_t *array, 
              float16_2_t  value, 
        const int32_t      num_elements
    ) {
    
     /**
     * Inplace subtraction of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be subtracted from. 
     *           float   *array_2: array to subtract from array_1.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaSubtract()
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, subtract array_2 element from array 1:
    if (tid < num_elements) 
	{    
        array[tid] -= value;
    }
}

extern "C" int32_t testCudaSubtract(
    const int32_t verbosity
    ) {
    
    /**
     * Test cudaSubtract function.
     * @param 
     *     const int32_t verbosity: Set verbosity of test.
     * @see cudaSubtract()
     * @return pass or error function.
     */
    
    float *array_1, *array_2;
    float *array_1_g, *array_2_g; 
    int32_t pass = true;
        
    // Allocate host memory:
    array_1   = (float*)malloc(sizeof(float) * TEST_LENGTH);
    array_2   = (float*)malloc(sizeof(float) * TEST_LENGTH);

    // Initialize host arrays:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        array_1[index] = 2.0f;
        array_2[index] = 1.0f;
    }

    // Allocate device memory:
    CUDA_CALL(cudaMalloc((void**)&array_1_g, sizeof(float) * TEST_LENGTH));
    CUDA_CALL(cudaMalloc((void**)&array_2_g, sizeof(float) * TEST_LENGTH));

    // Transfer data from host to device memory:
    CUDA_CALL(
        cudaMemcpy(
            array_1_g, 
            array_1, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyHostToDevice)
        );
    CUDA_CALL(
        cudaMemcpy(
            array_2_g, 
            array_2, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyHostToDevice)
        );

    // Executing kernel:
    const int32_t grid_size  = ((TEST_LENGTH + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaSubtract<<<grid_size,BLOCK_SIZE>>>(array_1_g, array_2_g, TEST_LENGTH);
        
    // Transfer data back to host memory:
    CUDA_CALL(
        cudaMemcpy(
            array_1, 
            array_1_g, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyDeviceToHost)
        );

    // Verification:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        pass *= (fabs(array_1[index] - array_2[index]) < MAX_ERR);
    }
    
    //Print result:
    if (verbosity >= 3) 
	{    
        printTestResult(pass, "cudaSubtract");
    }
    
    // Deallocate device memory:
    cudaFree(array_1_g);
    cudaFree(array_2_g);

    // Deallocate host memory:
    free(array_1); 
    free(array_2); 
    
    // Return pass value:
    return !pass;
}

extern "C" __global__ void cudaMultiply(
          float   *array_1, 
          float   *array_2, 
    const int32_t  num_elements
    ) {
    
    /**
     * Inplace multiplication of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be multiplied. 
     *           float   *array_2: array to multiply array_1 by.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaMultiply()
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, multiply array_1 element by array 2:
    if (tid < num_elements) 
	{    
        array_1[tid] *= array_2[tid];
    }
}

__global__ void cudaMultiply(
          float16_2_t *array_1, 
          float16_2_t *array_2, 
    const int32_t      num_elements
    ) {
    
    /**
     * Inplace multiplication of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be multiplied. 
     *           float   *array_2: array to multiply array_1 by.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaMultiply()
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, multiply array_1 element by array 2:
    if (tid < num_elements) 
	{    
        array_1[tid] *= array_2[tid];
    }
}

extern "C" int32_t hostCudaMultiply(
          float   *array_1, 
          float   *array_2, 
    const int32_t  num_elements
    ) {
    
    const int32_t grid_size = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaMultiply<<<grid_size,BLOCK_SIZE>>>
        (array_1, array_2, num_elements);
 
     return 0;
}

extern "C" int32_t hostCudaMultiply16(
          float16_2_t *array_1, 
          float16_2_t *array_2, 
    const int32_t      num_elements
    ) {
    
    const int32_t grid_size = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaMultiply<<<grid_size,BLOCK_SIZE>>>
        (array_1, array_2, num_elements);
 
     return 0;
}

extern "C" __global__ void cudaMultiplyByValue(
              float   *array, 
              float    value, 
        const int32_t  num_elements
    ) {
    
    /**
     * Inplace multiplication of a float array by a value using cuda GPU 
     * library.
     * @param 
     *           float   *array: array to be multiplied. 
     *           float    value: value to multiply array_1 by.
     *     const int32_t num_elements: num elements in array_1. 
     * @see 
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, multiply array element by value:
    if (tid < num_elements) 
	{    
        array[tid] *= value;
    }
}

extern "C" int32_t testCudaMultiply(
    const int32_t verbosity
    ) {
    
    /**
     * Test cudaMultiply function.
     * @param 
     *     const int32_t verbosity: Set verbosity of test.
     * @see cudaMultiply()
     * @return int32_t pass or error_function.
     */
    
    float *array_1, *array_2;
    float *array_1_g, *array_2_g; 
    int32_t pass = true;
        
    // Allocate host memory:
    array_1 = (float*)malloc(sizeof(float) * TEST_LENGTH);
    array_2 = (float*)malloc(sizeof(float) * TEST_LENGTH);

    // Initialize host arrays:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        array_1[index] = 2.0f;
        array_2[index] = 3.0f;
    }

    // Allocate device memory:
    CUDA_CALL(cudaMalloc((void**)&array_1_g, sizeof(float) * TEST_LENGTH));
    CUDA_CALL(cudaMalloc((void**)&array_2_g, sizeof(float) * TEST_LENGTH));

    // Transfer data from host to device memory:
    CUDA_CALL(
        cudaMemcpy(
            array_1_g, 
            array_1, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyHostToDevice)
        );
    CUDA_CALL(
        cudaMemcpy(
            array_2_g, 
            array_2, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyHostToDevice)
        );

    // Executing kernel:
    const int32_t grid_size  = ((TEST_LENGTH + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaMultiply<<<grid_size,BLOCK_SIZE>>>(array_1_g, array_2_g, TEST_LENGTH);
        
    // Transfer data back to host memory:
    CUDA_CALL(
        cudaMemcpy(
            array_1, 
            array_1_g, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyDeviceToHost)
        );

    // Verification:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        pass *= (fabs(array_1[index] - 6.0f) < MAX_ERR);
    }
    
    // Print result:
    if (verbosity >= 3) 
	{    
        printTestResult(pass, "cudaMultiply");
    }
    
    // Deallocate device memory:
    cudaFree(array_1_g);
    cudaFree(array_2_g);

    // Deallocate host memory:
    free(array_1); 
    free(array_2); 
    
    // Return pass value:
    return !pass;
}

extern "C" int32_t hostCudaMultiplyByValue(
              float   *array, 
              double   value, 
        const int32_t  num_elements
    ) {
    
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaMultiplyByValue<<<grid_size,BLOCK_SIZE>>>
        (array, (float)value, num_elements);
 
     return 0;
}

extern "C" __global__ void cudaMultiplyByValueMulti(
              float   *array, 
        const float   *value_array, 
        const int32_t  num_segments,
        const int32_t  num_elements_per_segment,
        const int32_t  total_num_elements
    ) {
    
    /**
     * Inplace division of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be divided. 
     *           float   *array_2: array to divide array_1 by.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaDivide()
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    const int32_t segment_index = (int32_t)
        floorf((float)tid / (float)num_elements_per_segment);
    
    // If not outside array bounds:
    if ((tid < total_num_elements) && (value_array[segment_index] != 0))
	{
        array[tid] *= value_array[segment_index];
    }
}

extern "C" __global__ void cudaMultiplyC(
              cuFloatComplex *array_1, 
              cuFloatComplex *array_2, 
        const int32_t         num_elements
    ) {
    
    /**
     * Inplace multiplication of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be multiplied. 
     *           float   *array_2: array to multiply array_1 by.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaMultiply()
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, multiply array_1 element by array 2:
    if (tid < num_elements) 
	{    
		array_1[tid] = cuCmulf(array_1[tid], array_2[tid]);
    }
}

__global__ void cudaDivide(
              float   *array_1, 
              float   *array_2, 
        const int32_t  num_elements
    ) {
    
    /**
     * Inplace division of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be divided. 
     *           float   *array_2: array to divide array_1 by.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaDivide()
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, divide array_1 element by array 2:
    if (tid < num_elements) 
	{
        array_1[tid] /= array_2[tid];
    }
}

__global__ void cudaDivide(
              double  *array_1, 
              double  *array_2, 
        const int32_t  num_elements
    ) {
    
    /**
     * Inplace division of two double arrays using cuda GPU library.
     * @param 
     *           double  *array_1: array to be divided. 
     *           double  *array_2: array to divide array_1 by.
     *     const int32_t  num_elements: num elements in array_1. 
     * @see testCudaDivide()
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, divide array_1 element by array 2:
    if (tid < num_elements) 
	{
        array_1[tid] /= array_2[tid];
    }
}

extern "C" int32_t testCudaDivide(
    const int32_t verbosity
    ) {
    
    /**
     * Test cudaDivide function.
     * @param 
     *     const int32_t verbosity: Set verbosity of test.
     * @see cudaDivide()
     * @return int32_t pass or error function.
     */
    
    float *array_1, *array_2;
    float *array_1_g, *array_2_g; 
    int32_t pass = true;
        
    // Allocate host memory:
    array_1   = (float*)malloc(sizeof(float) * TEST_LENGTH);
    array_2   = (float*)malloc(sizeof(float) * TEST_LENGTH);

    // Initialize host arrays:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        array_1[index] = 2.0f;
        array_2[index] = 3.0f;
    }

    // Allocate device memory:
    CUDA_CALL(cudaMalloc((void**)&array_1_g, sizeof(float) * TEST_LENGTH));
    CUDA_CALL(cudaMalloc((void**)&array_2_g, sizeof(float) * TEST_LENGTH));

    // Transfer data from host to device memory
    CUDA_CALL(
        cudaMemcpy(
            array_1_g, 
            array_1, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyHostToDevice)
        );
    CUDA_CALL(
        cudaMemcpy(
            array_2_g, 
            array_2, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyHostToDevice)
        );

    // Executing kernel:
    const int32_t grid_size = ((TEST_LENGTH + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaDivide<<<grid_size,BLOCK_SIZE>>>(array_1_g, array_2_g, TEST_LENGTH);
        
    // Transfer data back to host memory
    CUDA_CALL(
        cudaMemcpy(
            array_1, 
            array_1_g, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyDeviceToHost)
        );

    // Verification:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{        
        pass *= (fabs(array_1[index] - 2.0f/3.0f) < MAX_ERR);
    }
    
    //Print result:
    if (verbosity >= 3) 
	{    
        printTestResult(pass, "cudaDivide");
    }
    
    // Deallocate device memory:
    cudaFree(array_1_g);
    cudaFree(array_2_g);

    // Deallocate host memory:
    free(array_1); 
    free(array_2); 
    
    //Return pass value:
    return !pass;
}

__global__ void cudaDivideByValue(
              float   *array, 
              float    value,
        const int32_t  num_elements
    ) {
    
    /**
     * Inplace division of  float array by value using cuda GPU library.
     * @param 
     *           float   *array: array to be divided. 
     *           float    value: value to divide array by.
     *     const int32_t num_elements: num elements in array. 
     * @see testCudaDivideByValue()
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, divide array element by array 2:
    if (tid < num_elements) 
	{
        array[tid] /= value;
    }
}

__global__ void cudaDivideByValue(
              double  *array, 
              double   value,
        const int32_t  num_elements
    ) {
    
    /**
     * Inplace division of double array by value using cuda GPU library.
     * @param 
     *           double  *array: array to be divided. 
     *           double   value: value to divide array by.
     *     const int32_t  num_elements: num elements in array. 
     * @see testCudaDivideByValue()
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, divide array element by array 2:
    if (tid < num_elements) 
	{
        array[tid] /= value;
    }
}

extern "C" __global__ void cudaDivideByValueMulti(
              float   *array, 
        const float   *value_array, 
        const int32_t  num_segments,
        const int32_t  num_elements_per_segment,
        const int32_t  total_num_elements
    ) {
    
    /**
     * Inplace division of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be divided. 
     *           float   *array_2: array to divide array_1 by.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaDivide()
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    const int32_t segment_index = (int32_t)
        floorf((float)tid / (float)num_elements_per_segment);
    
    // If not outside array bounds, divide array_1 element by array 2:
    if ((tid < total_num_elements) && (value_array[segment_index] != 0))
	{
        array[tid] /= value_array[segment_index];
    }
}

extern "C" __global__ void cudaDivideByValueMulti16(
              float16_2_t *array, 
        const float16_2_t *value_array, 
        const int32_t      num_segments,
        const int32_t      num_elements_per_segment,
        const int32_t      total_num_elements
    ) {
    
    /**
     * Inplace division of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be divided. 
     *           float   *array_2: array to divide array_1 by.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaDivide()
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    const int32_t segment_index = (int32_t)
        floorf((float)tid / (float)num_elements_per_segment);
	
    // If not outside array bounds, divide array_1 element by array 2:
    if ((tid < total_num_elements) && __hbne2(value_array[segment_index], __float2half2_rn(0.0f)))
	{
        array[tid] /= value_array[segment_index];
    }
}

extern "C" __global__ void cudaLessThanEqualValue(
              float   *array, 
              float    value,
        const int32_t  num_elements
    ) {
    
    /**
     * Inplace conditioning of float array by value using cuda GPU library.
     * @param 
     *           float   *array: array to be conditioned. 
     *           float    value: value to divide array by.
     *     const int32_t num_elements: num elements in array. 
     * @see 
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, divide array_1 element by array 2:
    if (tid < num_elements) 
	{
        array[tid] = (float)(array[tid] <= value);
    }
}

__global__ void cudaRoundAway(
              float   *array, 
        const int32_t  num_elements
    ) {
    
    /**
     * Inplace conditioning of float array by value using cuda GPU library.
     * @param 
     *           float   *array: array to be conditioned. 
     *           float    value: value to divide array by.
     *     const int32_t num_elements: num elements in array. 
     * @see 
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, divide array_1 element by array 2:
    if (tid < num_elements) 
	{
        array[tid] = (array[tid] < 0) ? floorf(array[tid]) : ceilf(array[tid]);
    }
}

__global__ void cudaRoundAway(
              float16_2_t *array, 
        const int32_t      num_elements
    ) {
    
    /**
     * Inplace conditioning of float array by value using cuda GPU library.
     * @param 
     *           float   *array: array to be conditioned. 
     *           float    value: value to divide array by.
     *     const int32_t num_elements: num elements in array. 
     * @see 
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, divide array_1 element by array 2:
    if (tid < num_elements) 
	{
        array[tid] = (array[tid] < __float2half2_rn(0.0f)) ? 
            h2floor(array[tid]) : h2ceil(array[tid]);
    }
}

extern "C" int32_t testCudaDivideByValue(
    const int32_t verbosity
    ) {
    
    /**
     * Test cudaDivideByValue function.
     * @param 
     *     const int32_t verbosity: Set verbosity of test.
     * @see cudaDivideByValue()
     * @return int32_t pass or error function.
     */
    
    float *array_1;
    float *array_1_g;
    int32_t pass = true;
        
    // Allocate host memory:
    array_1 = (float*)malloc(sizeof(float) * TEST_LENGTH);

    // Initialize host arrays:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        array_1[index] = 2.0f;
    }

    // Allocate device memory:
    CUDA_CALL(cudaMalloc((void**)&array_1_g, sizeof(float) * TEST_LENGTH));

    // Transfer data from host to device memory
    CUDA_CALL(
        cudaMemcpy(
            array_1_g, 
            array_1, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyHostToDevice)
        );

    // Executing kernel:
    const int32_t grid_size  = ((TEST_LENGTH + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaDivideByValue<<<grid_size,BLOCK_SIZE>>>(array_1_g, 3.0f, TEST_LENGTH);
        
    // Transfer data back to host memory
    CUDA_CALL(
        cudaMemcpy(
            array_1, 
            array_1_g, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyDeviceToHost)
        );

    // Verification:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        pass *= (fabs(array_1[index] - 2.0f/3.0f) < MAX_ERR);
    }
    
    //Print result:
    if (verbosity >= 3) 
	{    
        printTestResult(pass, "cudaDivideByValue");
    }
    
    // Deallocate device memory:
    cudaFree(array_1_g);

    // Deallocate host memory:
    free(array_1); 
    
    //Return pass value:
    return !pass;
}

extern "C" __global__ void cudaExpf(
              float   *array, 
        const int32_t  num_elements
    ) {
    
    /**
     * Inplace multiplication of a float array by a value using cuda GPU 
     * library.
     * @param 
     *           float   *array: array to be multiplied. 
     *           float    value: value to multiply array_1 by.
     *     const int32_t num_elements: num elements in array_1. 
     * @see 
     * @return void
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, multiply array element by value:
    if (tid < num_elements) 
	{    
        array[tid] = expf(array[tid]);
    }
}

extern "C" __global__ void cudaACosF(
              float   *array, 
        const int32_t  num_elements
    ) {
    
    /**
     * Inplace acosf of float array by value using cuda GPU library.
     * @param 
     *           float   *array_1: array to be acosf(x). 
     *     const int32_t num_elements: num elements in array_1. 
     * @see 
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, divide array_1 element by array 2:
    if (tid < num_elements) 
	{
        array[tid] = acosf(array[tid]);
    }
}

extern "C" int32_t hostCudaACosF(
              float   *array, 
        const int32_t  num_elements
    ) {
    
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaACosF<<<grid_size,BLOCK_SIZE>>>
        (array, num_elements);
 
     return 0;
}

extern "C" int32_t initCurandGenerator(
    const int64_t            seed,
          curandGenerator_t *generator
    ) {
    
    /**
     * Initilise cuda random generator for cuda GPU library.
     * @param 
     *     const int32_t            seed: seed for random number generator.
     *           curandGenerator_t  generator: pointer to random number 
     *                                         generator.
     * @see
     * @return int32_t pass or error function.
     */

    CURAND_CALL(
        curandCreateGenerator(
            generator, 
            CURAND_RNG_PSEUDO_DEFAULT
        )
    );

    // Set seed:
    CURAND_CALL(
        curandSetPseudoRandomGeneratorSeed(
            *generator, 
            seed
        )
    );
          
    return 0;
}

__global__ void __cudaGenerateFloat16Array(
	const ushort2      *array,
	const int32_t      num_elements,
	      float16_2_t *output
    ) {
    
     //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, divide array_1 element by array 2:
    if (tid < num_elements) 
	{
        output[tid] =
			__floats2half2_rn((float) (array[tid].x-31), (float) (array[tid].y-31))
			/ __float2half2_rn(65504.0f);
    }
}


extern "C" int32_t cudaGenerateNewRandomArray16(
    const int32_t             num_elements,
          curandGenerator_t   generator,
          float16_2_t       **ret_array_g
    ) {
    
    ushort2 *array_g = NULL;
    CUDA_CALL(
        cudaMalloc(
            (void**)&array_g, 
            sizeof(uint32_t)*(size_t)num_elements
        )
    );
    
    CURAND_CALL(
        curandGenerate(
            generator, 
            (unsigned int *) array_g, 
            num_elements
        )
    );
	
	//cast to short2, short2 then to __half, __half
	    
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);    
    __cudaGenerateFloat16Array<<<grid_size,BLOCK_SIZE>>>(
		  array_g,
          num_elements,
		  (float16_2_t *)array_g
    );
        
    *ret_array_g = (float16_2_t *)array_g;
    return 0;
}

extern "C" int32_t cudaGenerateNewRandomArray(
    const int32_t             num_elements,
          curandGenerator_t   generator,
          float             **ret_array_g
    ) {
    
    /**
     * Generate array of random numbers between 0.0 and 1.0, 0.0 excluded, using 
     * cuda GPU library.
     * @param 
     *     const int32_t            num_elements: num elements in random array to be 
     *                                      generated. 
     *           curandGenerator_t  generator: random number generator.
     *           float             *array: array to fill with random numbers.
     * @see testGenerateRandomArray()
     * @return int32_t pass or error function.
     */
     
    float *array_g = NULL;
    CUDA_CALL(
        cudaMalloc(
            (void**)&array_g, 
            sizeof(float)*(size_t)num_elements
        )
    );

    //Generate n floats on device:
    CURAND_CALL(
        curandGenerateUniform(
            generator, 
            array_g, 
            num_elements
        )
     ); 
     
     *ret_array_g = array_g;
          
     return 0;
}

extern "C" int32_t cudaGenerateRandomArray(
    const int32_t            num_elements,
          curandGenerator_t  generator,
          float             *array_g
    ) {
    
    /**
     * Generate array of random numbers between 0.0 and 1.0, 0.0 excluded, using 
     * cuda GPU library.
     * @param 
     *     const int32_t            num_elements: num elements in random array to be 
     *                                      generated. 
     *           curandGenerator_t  generator: random number generator.
     *           float             *array: array to fill with random numbers.
     * @see testGenerateRandomArray()
     * @return int32_t pass or error function.
     */

    //Generate n floats on device:
    CURAND_CALL(
        curandGenerateUniform(
            generator, 
            array_g, 
            num_elements
        )
     ); 
          
     return 0;
}

extern "C" int32_t cudaGenerateRandomWeightedBoolArray(
    const int32_t            num_elements,
          curandGenerator_t  generator,
          double             weight,
          float             *array
    ) {
    
    /**
     * Generate array of boolean with probability of true == weight, using 
     * cuda GPU library.
     * @param 
     *     const int32_t            num_elements: num elements in random array to be 
     *                                      generated. 
     *           curandGenerator_t  generator: random number generator.
     *           double             weight: probability of true.
     *           float             *array: array to fill with random booleans.
     * @see testGenerateRandomArray()
     * @return int32_t pass or error function.
     */
    //Generate n floats on device:
    CURAND_CALL(
        curandGenerateUniform(
            generator, 
            array, 
            num_elements
        )
     ); 
     
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
     
     cudaLessThanEqualValue<<<grid_size,BLOCK_SIZE>>>(
         array,
         (float)weight,
         num_elements
    );
          
     return 0;
}

int32_t cudaGenerateRandomSignArray(
    const int32_t            num_elements,
          curandGenerator_t  generator,
          float             *array
    ) {
    
    /**
     * Generate array of boolean with probability of true == weight, using 
     * cuda GPU library.
     * @param 
     *     const int32_t            num_elements: num elements in random array to be 
     *                                      generated. 
     *           curandGenerator_t  generator: random number generator.
     *           double             weight: probability of true.
     *           float             *array: array to fill with random booleans.
     * @see testGenerateRandomArray()
     * @return int32_t pass or error function.
     */

    //Generate n floats on device:
    CURAND_CALL(
        curandGenerateUniform(
            generator, 
            array, 
            num_elements
        )
     ); 
     
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
     
    cudaSubtractValue<<<grid_size,BLOCK_SIZE>>>(
        array,
        0.5f,
        num_elements
    );
    
    cudaRoundAway<<<grid_size,BLOCK_SIZE>>>(
         array,
         num_elements
    );
          
     return 0;
}

int32_t cudaGenerateRandomSignArray(
    const int32_t            num_elements,
          curandGenerator_t  generator,
          float16_2_t       *array
    ) {
    
    /**
     * Generate array of boolean with probability of true == weight, using 
     * cuda GPU library.
     * @param 
     *     const int32_t            num_elements: num elements in random array to be 
     *                                      generated. 
     *           curandGenerator_t  generator: random number generator.
     *           double             weight: probability of true.
     *           float             *array: array to fill with random booleans.
     * @see testGenerateRandomArray()
     * @return int32_t pass or error function.
     */

    //Generate n floats on device:
    CURAND_CALL(
        curandGenerateUniform(
            generator, 
            (float*)array, 
            num_elements
        )
     ); 
     
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
     
    cudaSubtractValue<<<grid_size,BLOCK_SIZE>>>(
        array,
        __float2half2_rn(0.5),
        num_elements
    );
    
    cudaRoundAway<<<grid_size,BLOCK_SIZE>>>(
         array,
         num_elements
    );
          
     return 0;
}

typedef struct adjustMinMax{
    float        min;
    float        max;
    float16_2_t *ptr;
} adjust_min_max_s;

__device__ __forceinline__ void cudaAdjustRange(
    const float  min,
    const float  max,
          float *ptr               
    ) {
    
    *ptr *= (max - min);
    *ptr += min;
}

__device__ __forceinline__ void cudaAdjustRange(
    const float        min,
    const float        max,
          float16_2_t *ptr               
    ) {
    
	
	
    *ptr *= (__float2half2_rn(max) - __float2half2_rn(min));
    *ptr += __float2half2_rn(min);
}

extern "C" int32_t cudaGenerateRandomArrayBetween(
    const int32_t            num_elements,
          curandGenerator_t  generator,
          double             min,
          double             max,
          float             *array_g
    ) {
    
    /**
     * Generate array of random numbers between 0.0 and 1.0, 0.0 excluded, using 
     * cuda GPU library.
     * @param 
     *     const int32_t            num_elements: num elements in random array to be 
     *                                      generated. 
     *           curandGenerator_t  generator: random number generator.
     *           float             *array: array to fill with random numbers.
     * @see testGenerateRandomArray()
     * @return int32_t pass or error function.
     */

    //Generate n floats on device:
    CURAND_CALL(
        curandGenerateUniform(
            generator, 
            array_g, 
            num_elements
        )
     ); 
     
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaMultiplyByValue<<<grid_size,BLOCK_SIZE>>>
        (array_g, (float) (max - min), num_elements);
    cudaAddValue<<<grid_size,BLOCK_SIZE>>>
        (array_g, (float) min, num_elements);
    
    return 0;
}

extern "C" int32_t cudaGenerateRandomArrayBetweenArrays(
    const int32_t            num_elements,
          curandGenerator_t  generator,
          float             *min_array,
          float             *max_array,
          float             *array_g
    ) {
    
    /**
     * Generate array of random numbers between 0.0 and 1.0, 0.0 excluded, using 
     * cuda GPU library.
     * @param 
     *     const int32_t            num_elements: num elements in random array to be 
     *                                      generated. 
     *           curandGenerator_t  generator: random number generator.
     *           float             *array: array to fill with random numbers.
     * @see testGenerateRandomArray()
     * @return int32_t pass or error function.
     */

    //Generate n floats on device:
    CURAND_CALL(
        curandGenerateUniform(
            generator, 
            array_g, 
            num_elements
        )
     ); 
     
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    
    cudaSubtract<<<grid_size,BLOCK_SIZE>>>
        (max_array, min_array, num_elements);
    cudaMultiply<<<grid_size,BLOCK_SIZE>>>
        (array_g, max_array, num_elements);
    cudaAdd<<<grid_size,BLOCK_SIZE>>>
        (array_g, min_array, num_elements);
          
    return 0;
}

extern "C" int32_t cudaGenerateRandomLogArrayBetween(
    const int32_t            num_elements,
          curandGenerator_t  generator,
          double             min,
          double             max,
          float             *array_g
    ) {
    
    /**
     * Generate array of random numbers between 0.0 and 1.0, 0.0 excluded, using 
     * cuda GPU library.
     * @param 
     *     const int32_t            num_elements: num elements in random array to be 
     *                                      generated. 
     *           curandGenerator_t  generator: random number generator.
     *           float             *array: array to fill with random numbers.
     * @see testGenerateRandomArray()
     * @return int32_t pass or error function.
     */
     
    min = log(min);
    max = log(max);

    //Generate n floats on device:
    CURAND_CALL(
        curandGenerateUniform(
            generator, 
            array_g, 
            num_elements
        )
     ); 
     
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaMultiplyByValue<<<grid_size,BLOCK_SIZE>>>
        (array_g, (float) (max - min), num_elements);
    cudaAddValue<<<grid_size,BLOCK_SIZE>>>
        (array_g, (float) min, num_elements);
    cudaExpf<<<grid_size,BLOCK_SIZE>>>
        (array_g, num_elements);
          
    return 0;
}

extern "C" __global__ void cudaFloorArray(
		  float   *array,
	const int32_t  num_elements
    ) {
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, floor array value:
    if (tid < num_elements) 
	{    
        array[tid] = floorf(array[tid]);
    }
}

extern "C" int32_t cudaGenerateRandomIntArrayBetween(
    const int32_t            num_elements,
          curandGenerator_t  generator,
          double             min,
          double             max,
          float             *array
    ) {
    
    /**
     * Generate array of random numbers between 0.0 and 1.0, 0.0 excluded, using 
     * cuda GPU library.
     * @param 
     *     const int32_t            num_elements: num elements in random array to be 
     *                                      generated. 
     *           curandGenerator_t  generator: random number generator.
     *           float             *array: array to fill with random numbers.
     * @see testGenerateRandomArray()
     * @return int32_t pass or error function.
     */
     
     max++;

    //Generate n floats on device:
    CURAND_CALL(
        curandGenerateUniform(
            generator, 
            array, 
            num_elements
        )
     ); 
     
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaMultiplyByValue<<<grid_size,BLOCK_SIZE>>>
        (array, (float) (max - min), num_elements);
    cudaAddValue<<<grid_size,BLOCK_SIZE>>>
        (array, (float) min, num_elements);
    cudaFloorArray<<<grid_size,BLOCK_SIZE>>>(
		  array, num_elements);
          
    return 0;
}

extern "C" __global__ void cudaRoundArray(
		  float   *array,
	const int32_t  num_elements
    ) {
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, round array value:
    if (tid < num_elements) 
	{    
        array[tid] = roundf(array[tid]);
    }
}

extern "C" __global__ void cudaClipArray(
		  float   *array,
	const int32_t  num_elements,
          float    min,
          float    max
    ) {
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, clip array
    if (tid < num_elements) 
	{    
        array[tid] = array[tid]
                       * ((array[tid]>min)&&(array[tid]<max))
                   + max
                       *(array[tid]>=max)
                   + min
                       *(array[tid]<=min);
    }
}

extern "C" __global__ void cudaClipArrayInt(
		  int32_t *array,
	const int32_t  num_elements,
          int32_t  min,
          int32_t  max
    ) {
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, clip array
    if (tid < num_elements) 
	{    
        array[tid] = (array[tid]
                       * ((array[tid]>=min)&&(array[tid]<=max))
                   + max
                       *(array[tid]>max)
                   + min
                       *(array[tid]<min))
                    *!(min > max);
    }
}

extern "C" __global__ void castToInt(
          float   *float_array,
          int32_t *int_array,
	const int32_t  num_elements
    ) {
    
    /**
     * Inplace addition of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be added to. 
     *           float   *array_2: array to add to array_1.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaAdd()
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    
    //If not outside array bounds, add array_2 element to array 1:
    if (tid < num_elements) 
	{    
        int_array[tid] = (int32_t) float_array[tid];
    }
}

extern "C" int32_t castToIntHost(
          float    *array_g,
    const int32_t   num_elements,
          int32_t **ret_array_g
    ) {
    
    int32_t *int_array_g = NULL;
    
    CUDA_CALL(cudaMalloc((void**)&int_array_g, sizeof(int32_t) * num_elements));
    
    const int32_t grid_size  = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    
    castToInt<<<grid_size,BLOCK_SIZE>>>(
		  array_g,
          int_array_g,
          num_elements
    );
    
    cudaFree(array_g);
    
    *ret_array_g = int_array_g;
    
    return 0;
};

extern "C" __global__ void dilateToInt(
		  float   *array_1,
          int32_t *array_2,
	const int32_t  num_elements,
    const int32_t  dilation
    ) {
    
    /**
     * Inplace addition of two float arrays using cuda GPU library.
     * @param 
     *           float   *array_1: array to be added to. 
     *           float   *array_2: array to add to array_1.
     *     const int32_t num_elements: num elements in array_1. 
     * @see testCudaAdd()
     * @return void
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, add array_2 element to array 1:
    if (tid < num_elements) 
	{    
        array_2[tid*dilation] = (int32_t) array_1[tid];
    }
}

extern "C" int32_t returnFloatArrayToHost(
    const float    *array_g,
    const int32_t   num_elements,
          float   **ret_array
    ) {
    
    float *array = (float*)malloc(sizeof(float)*num_elements);
    CUDA_CALL(
        cudaMemcpy(
            array, 
            array_g, 
            sizeof(float) * num_elements, 
            cudaMemcpyDeviceToHost
        )
    );          
    
    *ret_array = array;
    
    return 0;
};

extern "C" int32_t returnArrayToHost(
    const float    *array_g,
    const int32_t   num_elements,
          float   **ret_array
    ) {
    
    size_t size = sizeof(float)*(size_t)num_elements;
    float *array = (float*)malloc(size);
    CUDA_CALL(
        cudaMemcpy(
            array, 
            array_g, 
            size, 
            cudaMemcpyDeviceToHost
        )
    );          
    
    *ret_array = array;
    
    return 0;
};

extern "C" int32_t returnIntArrayToHost(
    const int32_t  *array_g,
    const int32_t   num_elements,
          int32_t **ret_array
    ) {
    
    int32_t *array = (int32_t*)malloc(sizeof(int32_t)*num_elements);
    CUDA_CALL(
        cudaMemcpy(
            array, 
            array_g, 
            sizeof(int32_t) * num_elements, 
            cudaMemcpyDeviceToHost
        )
    );          
    
    *ret_array = array;
    
    return 0;
};

extern "C" int32_t cudaAllocateDeviceMemory(
	const size_t    size,
    const int32_t   num_elements,
          void    **ret_array
    ) {
    
    // Allocate device memory:
    void *array = NULL; size_t total_size = size*(size_t)num_elements;
    
    CUDA_CALL(cudaMalloc(&array, total_size));
    
    *ret_array = array;
    return 0;
}

extern "C" int32_t cudaCallocateDeviceMemory(
    const int32_t   num_elements,
	const size_t    size,
          void    **ret_array
    ) {
    
    // Allocate device memory:
    void *array = NULL; size_t total_size = size*(size_t)num_elements;
    
    CUDA_CALL(cudaMalloc(&array, total_size));
    CUDA_CALL(cudaMemset(array, 0, total_size));
    
    *ret_array = array;
    return 0;
}

extern "C" int32_t cudaCallocateDeviceMemoryInt(
    const int32_t   num_elements,
          int32_t **ret_array
    ) {
    
    // Allocate device memory:
    int32_t *array = NULL; size_t size = sizeof(int32_t)*num_elements;
    CUDA_CALL(cudaMalloc((void**)&array, size));
    CUDA_CALL(cudaMemset((void* ) array, 0, size));
    
    *ret_array = array;
    return 0;
}


extern "C" int32_t cudaAllocateIntDeviceMemory(
    const int32_t   num_elements,
          int32_t **ret_array
    ) {
    
    // Allocate device memory:
    int32_t *array;    
    CUDA_CALL(cudaMalloc((void**)&array, sizeof(int32_t)*num_elements));
    
    *ret_array = array;
    return 0;
}
    
extern "C" int32_t cudaGenerateRandomNormalArray(
    const int32_t            num_elements,
          curandGenerator_t  generator,
		  float              mean,
		  float              std_deviation,
          float             *array_g

    ) {
    
    /**
     * Generate array of random numbers between 0.0 and 1.0, 0.0 excluded, using 
     * cuda GPU library.
     * @param 
     *     const int32_t            num_elements: num elements in random array to be 
     *                                      generated. 
     *           curandGenerator_t  generator: random number generator.
     *           float             *array: array to fill with random numbers.
     * @see testGenerateRandomArray()
     * @return int32_t pass or error function.
     */

    // Generate n floats on device:
    CURAND_CALL(
        curandGenerateNormal(
            generator, 
            array_g, 
            num_elements,
			mean,
			std_deviation
        )
     ); 
          
     return 0;
}

extern "C" int32_t cudaGenerateNormalIntArray(
    const int32_t             num_elements,
          curandGenerator_t   generator,
          double              mean,
          double              std_deviation,
          double              min,
          double              max,
          float              *array
    ) {
        
    cudaGenerateRandomNormalArray(
          num_elements,
          generator,
          (float) mean,
          (float) std_deviation,
          array
    );
        
    const int32_t grid_size = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    
    cudaRoundArray<<<grid_size,BLOCK_SIZE>>>(
		  array,
          num_elements
    );
    
    cudaClipArray<<<grid_size,BLOCK_SIZE>>>(
		  array,
	      num_elements,
          (float) min,
          (float) max
    );
        
    return 0;
}

extern "C" int32_t testCudaGenerateRandomArray(
    const int32_t verbosity
    ) {
    
    /**
     * Test cudaGenerateRandomArray function.
     * @param 
     *     const int32_t verbosity: Set verbosity of test.
     * @see cudaDivide()
     * @return int32_t pass or error function.
     */
        
    int32_t pass = true;
    
    // Allocate host memory:
    float *array = (float*)malloc(sizeof(float) * TEST_LENGTH);
    
    // Allocate device memory:
    float *array_g;    
    CUDA_CALL(cudaMalloc((void**)&array_g, sizeof(float) * TEST_LENGTH));
    
    // Create pseudo-random number generator:
    curandGenerator_t  generator;
    CURAND_CALL(
        curandCreateGenerator(
            &generator, 
            CURAND_RNG_PSEUDO_DEFAULT
        )
    );
    
    // Set seed:
    CURAND_CALL(
        curandSetPseudoRandomGeneratorSeed(
            generator, 
            1234ULL
        )
    );
    
    // Generate n floats on device:
    cudaGenerateRandomNormalArray(TEST_LENGTH, generator, 1.0f, 0.0f, array_g);
     
    // Transfer data back to host memory
    CUDA_CALL(
        cudaMemcpy(
            array, 
            array_g, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyDeviceToHost)
        );

    // Verification:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        pass *= ((array[index] > 0.0f) && (array[index] <= 1.0f));
    }
    
    // Print result:
    if (verbosity >= 3) 
	{    
        printTestResult(pass, "cudaGenerateRandomArray");
    }
    
    // Deallocate device memory:
    cudaFree(array_g);

    // Deallocate host memory:
    free(array); 
     
    return !pass;
}

extern "C" int32_t cudaRfft(
    const int32_t         num_elements,
    const int32_t         num_transforms,
          cuFloatComplex *array
    ) {
    
    /**
     * Perform inplace rfft on cufftComlex data array.
     * @param 
     *     const int32_t             num_elements: num elements in array to be run through 
     *                                       rfft. 
     *     const int32_t             num_transforms: num rffts to be performed.
     *           cuFloatComplex       *array: array on which to perform rfft.
     * @see testCudaRfft()
     * @return int32_t 0 or error function.
     */
	
    // Create cufft plan:
    cufftHandle   plan;
    if (cufftPlan1d(&plan, num_elements, CUFFT_R2C, num_transforms) != CUFFT_SUCCESS){
        fprintf(stderr, "CUFFT error: Plan creation failed.\n");
        return EXIT_FAILURE;	
    }	

    // Use the CUFFT plan to transform the signal in place:
    if (cufftExecR2C(plan, (cufftReal*)array, array) != CUFFT_SUCCESS){
        fprintf(stderr, "CUFFT error: ExecC2C Forward failed.\n");
        return EXIT_FAILURE;	
    }
    
    // Error checking:
    if (cudaDeviceSynchronize() != cudaSuccess){
        fprintf(stderr, "Cuda error: Failed to synchronize.\n");
        return EXIT_FAILURE;	
    }
    
    // Deallocate plan memory:
    cufftDestroy(plan);
    
    return 0;
}

#include <cfloat> // for FLT_MAX and FLT_MIN

__device__ float d_min = 1e38; // Approximate equivalent to FLT_MAX
__device__ float d_max = -1e38; // Approximate equivalent to FLT_MIN
__device__ float d_sum = 0.0f;
__device__ int d_count = 0;

__device__ void atomicMin(float* address, float val)
{
    int *address_as_int = (int*)address;
    int old = *address_as_int, assumed;
    while (val < __int_as_float(old)) {
        assumed = old;
        old = atomicCAS(address_as_int, assumed, __float_as_int(val));
    }
}

__device__ void atomicMax(float* address, float val)
{
    int *address_as_int = (int*)address;
    int old = *address_as_int, assumed;
    while (val > __int_as_float(old)) {
        assumed = old;
        old = atomicCAS(address_as_int, assumed, __float_as_int(val));
    }
}

__global__ void compareArrays(float *arr1, float *arr2, bool *areEqual, int size) {
    int idx = threadIdx.x + blockDim.x * blockIdx.x;
    float abs_diff;

    if (idx < size) {
        abs_diff = fabsf(arr1[idx] - arr2[idx]);
        atomicMin(&d_min, abs_diff);
        atomicMax(&d_max, abs_diff);
        atomicAdd(&d_sum, abs_diff);
        atomicAdd(&d_count, 1);
        
        if (arr1[idx] != arr2[idx]) {
            *areEqual = false;
        }
    }
}

void compareDeviceArrays(float *d_arr1, float *d_arr2, int N) {
    bool areEqual = true;
    bool *d_areEqual;

    // Allocate device memory for d_areEqual
    cudaMalloc((void**)&d_areEqual, sizeof(bool));
    cudaMemcpy(d_areEqual, &areEqual, sizeof(bool), cudaMemcpyHostToDevice);

    // Execute the comparison kernel
    int numBlocks = (N + 255) / 256;
    compareArrays<<<numBlocks, 256>>>(d_arr1, d_arr2, d_areEqual, N);
    
    // Wait for all threads to complete
    cudaDeviceSynchronize();

    // Copy the areEqual value back to host
    cudaMemcpy(&areEqual, d_areEqual, sizeof(bool), cudaMemcpyDeviceToHost);

    // Print whether arrays are equal or not
    if (areEqual) {
        printf("Arrays are equal.\n");
    } else {
        printf("Arrays are not equal.\n");
    }

    // Declare variables to hold device results
    float h_min, h_max, h_avg;
    int h_count;

    // Copy device results back to host
    cudaMemcpyFromSymbol(&h_min, d_min, sizeof(float), 0, cudaMemcpyDeviceToHost);
    cudaMemcpyFromSymbol(&h_max, d_max, sizeof(float), 0, cudaMemcpyDeviceToHost);
    cudaMemcpyFromSymbol(&h_avg, d_sum, sizeof(float), 0, cudaMemcpyDeviceToHost);
    cudaMemcpyFromSymbol(&h_count, d_count, sizeof(int), 0, cudaMemcpyDeviceToHost);

    // Calculate average
    if(h_count != 0) {
        h_avg /= (float)h_count;
    } else {
        h_avg = 0;
    }

    // Print min, max and average absolute differences
    printf("Min Abs Diff: %e, Max Abs Diff: %e, Avg Abs Diff: %e\n", h_min, h_max, h_avg);

    // Free device memory
    cudaFree(d_areEqual);
}

extern "C" int32_t cudaIRfft(
    const int32_t         num_elements,
    const int32_t         num_transforms,
	      double          normalisation_factor,
          cuFloatComplex *input,
          cufftReal      *output
    ) {
    
    /**
	 * Perform inplace irfft on cufftComlex data array.
     * @param 
     *     const int32_t             num_elements: num elements in array to be run through 
     *                                       rfft. 
     *     const int32_t             num_transforms: num rffts to be performed.
     *           cuFloatComplex       *array: array on which to perform irfft.
     * @see testCudaRfft()
     * @return 0 or error function.
     */
	
	int rank = 1;                           // --- 1D FFTs
	int n[] = { num_elements };             // --- Size of the Fourier transform
	int inembed[] = { 0 };                  // --- Input size with pitch (ignored for 1D transforms)
	int onembed[] = { 0 };                  // --- Output size with pitch (ignored for 1D transforms)
	int istride = 1, ostride = 1;           // --- Distance between two successive input/output elements
	int odist = 2*num_elements - 2, idist = num_elements; // --- Distance between batches
	int batch = num_transforms;             // --- Number of batched executions
    
    cufftHandle plan;
    if (
		cufftPlanMany(
			&plan, rank, n, 
			inembed, istride, idist,
			onembed, ostride, odist, CUFFT_C2R, batch
		) != CUFFT_SUCCESS)
    {
        fprintf(stderr, "CUIFFT error: Plan creation failed. \n");
        return EXIT_FAILURE;	
    }	

    /* Use the CUFFT plan to transform the signal in place. */
    if (
        cufftExecC2R(
            plan, 
            input, 
            (cufftReal*)output) 
        != CUFFT_SUCCESS)
    {
        fprintf(stderr, "CUFFT error: ExecC2C Forward failed. \n");
        return EXIT_FAILURE;	
    }
    
    if (cudaDeviceSynchronize() != cudaSuccess)
    {
        fprintf(stderr, "Cuda error: Failed to synchronize. \n");
        return EXIT_FAILURE;	
    }
    
    // Dividing by num_elements to return to orignal value:
    const int32_t grid_size = ((odist*num_transforms + BLOCK_SIZE - 1) / BLOCK_SIZE);
    
    cudaDivideByValue<<<grid_size,BLOCK_SIZE>>>(
        (float*)output, 
        (float)normalisation_factor, 
        odist*num_transforms
    );

    cufftDestroy(plan);
    
    return 0;
}

extern "C" int32_t cudaInterlacedIRFFT(
    const int32_t          num_elements,
	      double           normalisation_factor,
          cuFloatComplex  *input,
          float          **ret_output
    ) {
    
    /**
	 * Perform inplace irfft on cufftComlex input array.
     * @param 
     *     const int32_t             num_elements: num elements in array to be run through 
     *                                       rfft. 
     *     const int32_t             num_transforms: num rffts to be performed.
     *           cuFloatComplex       *array: array on which to perform irfft.
     * @see testCudaRfft()
     * @return 0 or error function.
     */
	
	int rank      = 1;              // --- 1D FFTs
	int n[]       = {num_elements}; // --- Size of the Fourier transform
	int inembed[] = {0};            // --- Input size with pitch (ignored for 1D transforms)
	int onembed[] = {0};                  // --- Output size with pitch (ignored for 1D transforms)
	int istride   = 2, ostride = 2; // --- Distance between two successive input/output elements
	int idist     = 1, odist = 1;   // --- Distance between batches
	int batch     = 2;               // --- Number of batched executions
    
    cufftHandle plan; 
    cufftResult result = 
        cufftPlanMany(
			&plan, rank, n, 
			inembed, istride, idist,
			onembed, ostride, odist, CUFFT_C2R, batch
		);
    
    if (result != CUFFT_SUCCESS)
    {
        fprintf(
            stderr, 
            "%s:\n"
            "CUIFFT error: Plan creation failed! Error code: %s \n", 
            __func__,
            _cufftGetErrorEnum(result)
        );
        return EXIT_FAILURE;	
    }	

    // Use the CUFFT plan to transform the signal in place:
    
    float *output;
    CUDA_CALL(cudaMalloc((void**)&output, sizeof(float) * num_elements * 2));

    result = 
        cufftExecC2R(
            plan, 
            (cufftComplex*)input, 
            (cufftReal*)output
        );
    
    cudaDeviceSynchronize();
    cufftDestroy(plan);
    cudaFree(input);
    
    if (result != CUFFT_SUCCESS)
    {
        fprintf(
            stderr, 
            "%s:\n"
            "CUIFFT error: xecC2C Forward failed! Error code: %s \n", 
            __func__,
            _cufftGetErrorEnum(result)
        );

        return EXIT_FAILURE;	
    }

    if (cudaDeviceSynchronize() != cudaSuccess)
    {
        fprintf(stderr, "Cuda error: Failed to synchronize. \n");
        return EXIT_FAILURE;	
    }
	
    // Dividing by num_elements to return to orignal value:
    const int32_t grid_size = ((num_elements*2 + BLOCK_SIZE - 1) / BLOCK_SIZE);
    
    cudaDivideByValue<<<grid_size,BLOCK_SIZE>>>(
        output, 
        (float)normalisation_factor, 
        num_elements*2
    );
    
    *ret_output = output;
    
    return 0;
}

extern "C" int32_t cudaIRfft64(
    const int32_t         num_elements,
    const int32_t         num_transforms,
	      double          normalisation_factor,
          cuDoubleComplex *data
    ) {
    
    /**
	 * Perform inplace irfft on cufftComlex data array.
     * @param 
     *     const int32_t             num_elements: num elements in array to be run through 
     *                                       rfft. 
     *     const int32_t             num_transforms: num rffts to be performed.
     *           cuFloatComplex       *array: array on which to perform irfft.
     * @see testCudaRfft()
     * @return 0 or error function.
     */
	
	int rank = 1;                           // --- 1D FFTs
	int n[] = { num_elements };             // --- Size of the Fourier transform
	int inembed[] = { 0 };                  // --- Input size with pitch (ignored for 1D transforms)
	int onembed[] = { 0 };                  // --- Output size with pitch (ignored for 1D transforms)
	int istride = 1, ostride = 1;           // --- Distance between two successive input/output elements
	int odist = 2*num_elements, idist = num_elements; // --- Distance between batches
	int batch = num_transforms;             // --- Number of batched executions
    
    cufftHandle plan;
    if (
		cufftPlanMany(
			&plan, rank, n, 
			inembed, istride, idist,
			onembed, ostride, odist, CUFFT_Z2D, batch
		) != CUFFT_SUCCESS)
    {
        fprintf(stderr, "CUIFFT error: Plan creation failed. \n");
        return EXIT_FAILURE;	
    }	

    /* Use the CUFFT plan to transform the signal in place. */
    if (
        cufftExecZ2D(
            plan, 
            (cufftDoubleComplex*)data, 
            (cufftDoubleReal*)data) 
        != CUFFT_SUCCESS)
    {
        fprintf(stderr, "CUFFT error: ExecC2C Forward failed. \n");
        return EXIT_FAILURE;	
    }

    if (cudaDeviceSynchronize() != cudaSuccess)
    {
        fprintf(stderr, "Cuda error: Failed to synchronize. \n");
        return EXIT_FAILURE;	
    }
	
    // Dividing by num_elements to return to orignal value:
    const int32_t grid_size = ((num_elements*num_transforms*2 + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaDivideByValue<<<grid_size,BLOCK_SIZE>>>(
        (double*)data, 
        normalisation_factor, 
        num_elements*num_transforms*2
    );

    cufftDestroy(plan);
    
    return 0;
}

/*
extern "C" int32_t testCudaRfftMany(
    const int32_t verbosity
    ) {
    
    
      // Test cudaRfft and cudaIRfft functions.
      // @param 
      //    const int32_t verbosity: Set verbosity of test.
      // @see cudaRfft(), cudaIRfft()
      // @return int32_t pass or error function.
    
    bool pass = true;
    
    // Allocate host memory:
    float *original_array    = (float*)malloc(sizeof(float) * TEST_LENGTH);
    float *transformed_array = (float*)malloc(sizeof(float) * TEST_LENGTH);
    
    // Allocate device memory:
    cuFloatComplex *array_g;
    CUDA_CALL(
        cudaMalloc(
            (void**)&array_g, 
            sizeof(cuFloatComplex) * TEST_LENGTH)
        );
    
    // Create pseudo-random number generator:
    curandGenerator_t  generator;
    CURAND_CALL(
        curandCreateGenerator(
            &generator, 
            CURAND_RNG_PSEUDO_DEFAULT
        )
    );
    
    // Generate random Array for testing:
    cudaGenerateRandomArray(TEST_LENGTH, generator, (float*) array_g);
    
    // Save array for comparison:
    CUDA_CALL(
        cudaMemcpy(
            original_array, 
            (float*) array_g, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyDeviceToHost)
        );
    
    // Run rfft function:
    pass *= !cudaRfft(TEST_LENGTH, 1, array_g);
    
    // Run ifft function:
    pass *= !cudaIRfft(TEST_LENGTH, 1, TEST_LENGTH, array_g);
        
    // Copy array back to host:
    CUDA_CALL(
        cudaMemcpy(
            transformed_array, 
            (float*) array_g, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyDeviceToHost)
        );
    
    // Verification:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        pass *= 
            (fabs(transformed_array[index] - original_array[index]) < MAX_ERR);
    }
    
    // Print result:
    if (verbosity >= 3) 
	{    
        printTestResult(pass, "cudaRfft");
    }
    
    // Deallocate device memory:
    cudaFree(array_g);
    
    // Deallocate host memory:
    free(original_array);
    free(transformed_array);
    
    return !pass;
}
*/

extern "C" __global__ void cudaGenerateHammingWindow(
          float   *window, 
    const int32_t  num_elements
    ) {
	
	/**
     * Generate hamming window on the GPU.
     * @param 
     *           float    *window: pointer to generated window. 
     *     const int32_t   num_elements: num elements in generated window.
     * @see testCudaGenerateHammingWindow()
     * @return void.
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, generate window:
    if (tid < num_elements) 
	{
        window[tid] = 
             0.54f - 0.46f
            *cosf((2.0f*(float)M_PI*(float)tid)/((float)num_elements - 1.0f));
    }    
}

extern "C" __global__ void cudaGenerateHammingWindowC(
          cuFloatComplex *window, 
    const int32_t         num_elements
    ) {
	
	/**
     * Generate hamming window on the GPU.
     * @param 
     *           float    *window: pointer to generated window. 
     *     const int32_t   num_elements: num elements in generated window.
     * @see testCudaGenerateHammingWindow()
     * @return void.
     */
    
    //Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    //If not outside array bounds, generate window:
    if (tid < num_elements) 
	{
        window[tid].x = 0.54f - 0.46f*cosf((2*M_PI*tid)/(num_elements-1));
		window[tid].y = 0.0f;
    }    
}


extern "C" void generateHammingWindHost(
    const int32_t   intervals, 
          float   **data_ret
    ) {
	
	/**
     * Generate hamming window on the CPU. TEMP: Replace with maths.h
     * @param 
     *     const int32_t     intervals: num elements in generated window.
     *     const float    ** data_ret: pointer to generated window
     * @see testCudaGenerateHammingWindow()
     * @return void.
     */

	float *data = (float*) malloc(sizeof(float)*intervals);
	for (int32_t index = 0; index < intervals; ++index) 
	{
		data[index] = 
             0.54f - 0.46f
            *cosf((2.0f*(float)M_PI*(float)index)/((float)intervals-1));
	}
	*data_ret = data;
}

extern "C" int32_t testCudaGenerateHammingWindow(
    const int32_t verbosity
    ) {    
    /**
      * Test cudaGenerateHammingWindow function.
      * @param 
      *     const int32_t verbosity: Set verbosity of test.
      * @see cudaGenerateHammingWindow.
      * @return int32_t pass or error function.
      */
    
    bool pass = true;
    
    // Allocate host memory:
    float *window;
    float *cuda_window = (float*)malloc(sizeof(float) * TEST_LENGTH);
    
    // Allocate device memory:
    float *window_g;
    CUDA_CALL(cudaMalloc((void**)&window_g, sizeof(float) * TEST_LENGTH));
	
	// Execute kernal:
	const int32_t grid_size  = ((TEST_LENGTH + BLOCK_SIZE - 1) / BLOCK_SIZE);
	cudaGenerateHammingWindow<<<grid_size,BLOCK_SIZE>>>(window_g, TEST_LENGTH);
        
    // Copy array back to host:
    CUDA_CALL(
        cudaMemcpy(
            cuda_window, 
            (float*) window_g, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyDeviceToHost)
        );
    
	// Generate window on CPU:
	generateHammingWindHost(TEST_LENGTH, &window);
	
    // Verification:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        pass *= (fabs(cuda_window[index] - window[index]) < MAX_ERR);
    }
    
    // Print result:
    if (verbosity >= 3) 
	{    
        printTestResult(pass, "cudaGenerateHammingWindow");
    }
    
    // Deallocate device memory:
    cudaFree(window_g);
    
    // Deallocate host memory:
    free(cuda_window);
    free(window);
    
    return !pass;
}

extern "C" __global__ void cudaSetArrayValueInt(
          int32_t *array, 
	const int32_t  value,
    const int32_t  num_elements
    ) {
	
	/**
     * Set all values of cuda array to value on GPU.
     * @param 
	 *           float   *array: pointer to array to set.            
     *     const float    value: value to set array to.
     *     const int32_t  num_elements: num elements in array to be set to value.
     * @see testCudaSetArrayValue()
     * @return void.
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, generate window:
    if (tid < num_elements) 
	{
        array[tid] = value;
    }    
}

extern "C" void cudaSetArrayValueIntHost(
          int32_t *array, 
	const int32_t  value,
    const int32_t  num_elements
    ) {
	
	 /**
     * Sum array opposite pairs on GPU. 
     * i.e array[0] = array[0] + array[num_elements - 1].
	 * Umbrella function to prepare arguments for cudaFoldArray_()å
     * @param 
	 *           float   *array: pointer to array to fold.           
     *     const int32_t  num_elements:  num elements in array to be set to folded.
     * @see testCudaFoldArray()
     * @return void.
     */
    
	// Executing kernel:
    const int32_t grid_size = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaSetArrayValueInt<<<grid_size,BLOCK_SIZE>>>(array,value,num_elements);
}


extern "C" __global__ void cudaSetArrayValue(
          float   *array, 
	const float    value,
    const int32_t  num_elements
    ) {
	
	/**
     * Set all values of cuda array to value on GPU.
     * @param 
	 *           float   *array: pointer to array to set.            
     *     const float    value: value to set array to.
     *     const int32_t  num_elements: num elements in array to be set to value.
     * @see testCudaSetArrayValue()
     * @return void.
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, generate window:
    if (tid < num_elements) 
	{
        array[tid] = value;
    }    
}

extern "C" void cudaSetArrayValueHost(
          float   *array, 
	const float    value,
    const int32_t  num_elements
    ) {
	
	 /**
     * Sum array opposite pairs on GPU. 
     * i.e array[0] = array[0] + array[num_elements - 1].
	 * Umbrella function to prepare arguments for cudaFoldArray_()å
     * @param 
	 *           float   *array: pointer to array to fold.           
     *     const int32_t  num_elements:  num elements in array to be set to folded.
     * @see testCudaFoldArray()
     * @return void.
     */
    
	// Executing kernel:
    const int32_t grid_size = ((num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaSetArrayValue<<<grid_size,BLOCK_SIZE>>>(array,value,num_elements);
}

extern "C" int32_t cudaZeroArray(
          void    *array, 
	const size_t   size,
    const int32_t  num_elements
    ) {
	
	const size_t total_size = size*(size_t)num_elements;
	
	CUDA_CALL(cudaMemset(array, 0, total_size));
	
	return 0;
}

extern "C" __global__ void cudaFindMin(
          float   *array_1, 
	const float   *array_2,
    const int32_t  num_elements
    ) {
	
	/**
     * Set all values of cuda array to value on GPU.
     * @param 
	 *           float   *array: pointer to array to set.            
     *     const float    value: value to set array to.
     *     const int32_t  num_elements: num elements in array to be set to value.
     * @see testCudaSetArrayValue()
     * @return void.
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, generate window:
    if (tid < num_elements) 
	{
        array_1[tid] = 
            (array_1[tid] <= array_2[tid])*(array_1[tid])
            + (array_2[tid] < array_1[tid])*(array_2[tid]);
    }    
}

extern "C" __global__ void cudaSetArrayValueComplex(
          cuFloatComplex *array, 
	const cuFloatComplex  value,
    const int32_t         num_elements
    ) {
	
	/**
     * Set all values of cuda array to value on GPU.
     * @param 
	 *           float   *array: pointer to array to set.            
     *     const float    value: value to set array to.
     *     const int32_t  num_elements: num elements in array to be set to value.
     * @see testCudaSetArrayValue()
     * @return void.
     */
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, generate window:
    if (tid < num_elements) 
	{
        array[tid] = value;
    }    
}

extern "C" int32_t testCudaSetArrayValue(
    const int32_t verbosity
    ) {
    
    /**
      * Test cudaSetArrayValue function.
      * @param 
      *     const int32_t verbosity: Set verbosity of test.
      * @see cudaSetArrayValue()
      * @return int32_t pass or error function.
      */
    
    bool pass = true;
    
    // Allocate host memory:
    float *array = (float*)malloc(sizeof(float) * TEST_LENGTH);
    
    // Allocate device memory:
    float *array_g;
    CUDA_CALL(cudaMalloc((void**)&array_g, sizeof(float) * TEST_LENGTH));
	
	// Execute kernal:
	const int32_t grid_size  = ((TEST_LENGTH + BLOCK_SIZE - 1) / BLOCK_SIZE);
	cudaSetArrayValue<<<grid_size,BLOCK_SIZE>>>(
        array_g, 
        (float) M_PI, 
        TEST_LENGTH
    );
        
    // Copy array back to host:
    CUDA_CALL(
        cudaMemcpy(
            array, 
            (float*) array_g, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyDeviceToHost)
        );
	
    // Verification:
    for(int32_t index = 0; index < TEST_LENGTH; index++) 
	{    
        pass = pass && (fabs(array[index] - M_PI) < MAX_ERR);
    }
    
    // Print result:
    if (verbosity >= 3) 
    {    
        printTestResult(pass, "cudaSetArrayValue");
    }
    
    // Deallocate device memory:
    cudaFree(array_g);
    
    // Deallocate host memory:
    free(array);
    
    return !pass;
}

extern "C" int32_t cudaSumArray(
          float   *array, 
    const int32_t  num_elements,
          float   *ret_sum
	) {
	
	      int32_t grid_size = 0;
          
    float *last_value = &array[num_elements - 1]; 
    
    int32_t num_elements_remaining = num_elements;
    while(num_elements_remaining > 1)
    {
        num_elements_remaining = 
            (int32_t) floorf((float)num_elements_remaining / 2.0f);
        
        grid_size = ((num_elements_remaining + BLOCK_SIZE - 1) / BLOCK_SIZE);
        
        cudaAdd<<<grid_size,BLOCK_SIZE>>>
            (array, &array[num_elements_remaining], num_elements_remaining);
    }
    
    cudaMultiplyByValue<<<1,1>>>(last_value, (float) (num_elements % 2), 1);
    cudaAdd<<<1,1>>>(array, last_value, 1);
    
    float sum = 0;
	// Copy array back to host:
    CUDA_CALL(
        cudaMemcpy(
            &sum, 
            (float*) array, 
            sizeof(float), 
            cudaMemcpyDeviceToHost)
        );
    
    *ret_sum = sum;
    
    return 0;
}

extern "C" __global__ void cudaFoldArray_(
          float   *array , 
	const int32_t  half_num_elements,
    const int32_t  num_elements
	) {
	
    /**
     * Sum array opposite pairs on GPU. 
     * i.e array[0] = array[0] + array[num_elements - 1].
     * @param 
	 *           float   *array: pointer to array to fold.           
     *     const int32_t  half_num_elements: half the num elements in array
     *     const int32_t  num_elements:  num elements in array to be set to folded.
     * @see testCudaFoldArray()
     * @return void.
     */
	
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, add opposite side of array:
    if (tid < half_num_elements) {

        array[tid] += array[num_elements - 1 - tid];
		//Normalise:
		array[tid] *= 0.5;
		
		array[num_elements - 1 - tid] = array[tid];
    }    	
}

extern "C" void cudaFoldArray(
          float   *array, 
    const int32_t  num_elements
    ) {
	
	 /**
     * Sum array opposite pairs on GPU. 
     * i.e array[0] = array[0] + array[num_elements - 1].
	 * Umbrella function to prepare arguments for cudaFoldArray_()å
     * @param 
	 *           float   *array: pointer to array to fold.           
     *     const int32_t  num_elements:  num elements in array to be set to folded.
     * @see testCudaFoldArray()
     * @return void.
     */
    
	const int32_t half_num_elements = (num_elements/2);
	
	// Executing kernel:
    const int32_t grid_size = ((half_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaFoldArray_<<<grid_size,BLOCK_SIZE>>>(array,half_num_elements,num_elements);
}

extern "C" __global__ void cudaFoldArrayC_(
          cuFloatComplex *array , 
	const int32_t         half_num_elements,
    const int32_t         num_elements
	) {
	
    /**
     * Sum array opposite pairs on GPU. 
     * i.e array[0] = array[0] + array[num_elements - 1].
     * @param 
	 *           float   *array: pointer to array to fold.           
     *     const int32_t  half_num_elements: half the num elements in array
     *     const int32_t  num_elements:  num elements in array to be set to folded.
     * @see testCudaFoldArray()
     * @return void.
     */
	
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, add opposite side of array:
    if (tid < half_num_elements) 
    {
        array[tid].x += array[num_elements - 1 - tid].x;
		//Normalise:
		array[tid].x *= 0.5;
		
		array[num_elements - 1 - tid].x = array[tid].x;
    }    	
}

extern "C" void cudaFoldArrayC(
          cuFloatComplex *array, 
    const int32_t         num_elements
    ) {
	
	 /**
     * Sum array opposite pairs on GPU. 
     * i.e array[0] = array[0] + array[num_elements - 1].
	 * Umbrella function to prepare arguments for cudaFoldArray_()å
     * @param 
	 *           float   *array: pointer to array to fold.           
     *     const int32_t  num_elements:  num elements in array to be set to folded.
     * @see testCudaFoldArray()
     * @return void.
     */
    
	const int32_t half_num_elements = (num_elements/2);
	
	// Executing kernel:
    const int32_t grid_size = ((half_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaFoldArrayC_<<<grid_size,BLOCK_SIZE>>>(array,half_num_elements,num_elements);
}

extern "C" int32_t testCudaFoldArray(
    const int32_t verbosity
    ) {
    
    /**
      * Test cudaFoldArray function.
      * @param 
      *     const int32_t verbosity: Set verbosity of test.
      * @see cudaFoldArray().
      * @return int32_t pass or error function.
      */
    
    bool pass = true;
    
    // Allocate host memory:
    float *array = (float*)malloc(sizeof(float) * TEST_LENGTH);
    
    // Allocate device memory:
    float *array_g;
    CUDA_CALL(cudaMalloc((void**)&array_g, sizeof(float) * TEST_LENGTH));
	
	//Set array values:
	const int32_t grid_size  = ((TEST_LENGTH + BLOCK_SIZE - 1) / BLOCK_SIZE);
	cudaSetArrayValue<<<grid_size,BLOCK_SIZE>>>(
        array_g, 
        (float)M_PI, 
        TEST_LENGTH
    );
	
	// Execute kernal:
	cudaFoldArray(array_g, TEST_LENGTH);
        
    // Copy array back to host:
    CUDA_CALL(
        cudaMemcpy(
            array, 
            (float*) array_g, 
            sizeof(float) * TEST_LENGTH, 
            cudaMemcpyDeviceToHost)
        );
	
    // Verification:
    for(int32_t index = 0; index < TEST_LENGTH; index++) {
        
        pass = pass && (fabs(array[index] - M_PI) < MAX_ERR);
    }
    
    // Print result:
    if (verbosity >= 3) {
        
        printTestResult(pass, "cudaFoldArray");
    }
    
    // Deallocate device memory:
    cudaFree(array_g);
    
    // Deallocate host memory:
    free(array);
    
    return !pass;
}

__device__ __forceinline__ cuFloatComplex cudaCExpf(
	const cuFloatComplex z
	) {
	
	/**
     * Calculate the complex exponetial on the GPU.
     * @param 
	 *           cuFloatComplex z: value to convert  
     * @see testCudaCExpf()
     * @return exponetiated value.
     */

    cuFloatComplex res;
    const float t = expf (z.x);

    sincosf(z.y, &res.y, &res.x);

    res.x *= t;

    res.y *= t;

    return res;
}

__global__ void cudaCExpfArray(
          cuFloatComplex *array, 
    const int32_t         num_elements
    ) {
	
	 /**
     * Calculate the complex exponetials of an array on the GPU.
     * @param 
	 *           float   *array: pointer to array to find exp.           
     *     const int32_t  num_elements:  num elements in array to be set to folded.
     * @see testCudaExpf())
     * @return void.
     */
    
	// Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, calculate Expf:
    if (tid < num_elements) {
	
		array[tid] = cudaCExpf(array[tid]);
	}
}

extern "C" int32_t testCudaCExpf(
    const int32_t verbosity
    ) {
    
    /**
      * Test cudaCExpf function.
      * @param 
      *     const int32_t verbosity: Set verbosity of test.
      * @see cudaSetArrayValue()
      * @return int32_t pass or error function.
      */
    
          int32_t pass           = true;
	const int32_t num_test_cases = 10;
    
    // Allocate host memory:
    cuFloatComplex *array = 
        (cuFloatComplex*)malloc(sizeof(cuFloatComplex) * num_test_cases);
    
	const cuFloatComplex test_cases[] = 
	{
	(cuFloatComplex) {       0.0f ,         0.0f   },
	(cuFloatComplex) {       1.0f ,         1.0f   },
	(cuFloatComplex) {       2.0f ,         2.0f   },
	(cuFloatComplex) {       0.0f ,         1.0f   },
	(cuFloatComplex) {       1.0f ,         0.0f   },
	(cuFloatComplex) {      -2.12f,        -2.567f },
	(cuFloatComplex) {(float) M_PI, (float) M_SQRT2}, 
	(cuFloatComplex) {(float)-M_PI, (float) M_SQRT2}, 
	(cuFloatComplex) {(float) M_PI, (float)-M_SQRT2}, 
	(cuFloatComplex) {(float)-M_PI, (float)-M_SQRT2}, 
	};
	
	const cuFloatComplex awnsers[] = 
	{
	(cuFloatComplex) { 1.0f                  ,   0.0f                 },
	(cuFloatComplex) { 1.4686939399158851f   ,   2.2873552871788423f  },
	(cuFloatComplex) { -3.074932320639359f   ,   6.71884969742825f    },
	(cuFloatComplex) { 0.5403023058681398f   ,   0.8414709848078965f  },
	(cuFloatComplex) { 2.718281828459045f    ,   0.0f                 },
	(cuFloatComplex) {-0.10075620084061157f  ,  -0.06523633830372988f },
	(cuFloatComplex) { 3.608645108585477f    ,  22.85758814934434f    }, 
	(cuFloatComplex) { 0.0067389380793415376f,   0.042685236853867756f}, 
	(cuFloatComplex) { 3.608645108585477f    , -22.85758814934434f    }, 
	(cuFloatComplex) { 0.0067389380793415376f,  -0.042685236853867756f}, 
	};
	
    // Allocate device memory:
    cuFloatComplex *array_g;
    CUDA_CALL(
        cudaMalloc(
            (void**)&array_g, 
            sizeof(cuFloatComplex) * num_test_cases)
        );
	
	// Transfer data from host to device memory:
    CUDA_CALL(
        cudaMemcpy(
            array_g, 
            test_cases, 
            sizeof(cuFloatComplex) * num_test_cases, 
            cudaMemcpyHostToDevice)
        );
	
	// Execute kernal:
	const int32_t grid_size  = ((num_test_cases + BLOCK_SIZE - 1) / BLOCK_SIZE);
	cudaCExpfArray<<<grid_size,BLOCK_SIZE>>>(array_g, num_test_cases);			
	
    // Copy array back to host:
    CUDA_CALL(
        cudaMemcpy(
            array, 
            (float*) array_g, 
            sizeof(cuFloatComplex) * num_test_cases, 
            cudaMemcpyDeviceToHost)
        );
	
    // Verification:
    for(int32_t index = 0; index < num_test_cases; index++) 
	{    
        pass *= 
              ((fabs(array[index].x - awnsers[index].x) < MAX_ERR) 
            && (fabs(array[index].y - awnsers[index].y) < MAX_ERR)
            );
    }
    
    // Print result:
    if (verbosity >= 3) 
	{    
        printTestResult(pass, "testCudaCExpf");
    }
    
    // Deallocate device memory:
    cudaFree(array_g);
    
    // Deallocate host memory:
    free(array);
    
    return !pass;
}

extern "C" __global__ void cudaHetrodyneShift(
          cuFloatComplex *array, 
    const float           shift,
    const int32_t         num_elements
	) {
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, add opposite side of array:
    if (tid < num_elements) 
	{	
		cuFloatComplex shift_c  = (cuFloatComplex) {           0.0f, shift}; 
		cuFloatComplex tid_plus = (cuFloatComplex) {(float) (1+tid), 0.0f };
		
        array[tid] = cuCmulf(array[tid], cudaCExpf(cuCmulf(shift_c, tid_plus)));
    }    	
}

__global__ void cudaFindMax_(
          float   *array, 
		  float   *buffer,
	const int32_t  half_num_elements,
    const int32_t  num_elements
	) {
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, add opposite side of array:
    if (tid < half_num_elements) 
	{
        buffer[tid] = 
            (array[tid] >= array[num_elements - 1 - tid]) 
            ? array[tid] : array[num_elements - 1 - tid]; 
    }    	
}

__global__ void _setBufferValue(
          float *destination_array,
    const float *source_array
    ) {
    
    destination_array[0] = source_array[0];
}

extern "C" float cudaFindMaxHost(
          float   *array, 
    const int32_t  num_elements_o
	) {

	int32_t num_elements      = num_elements_o;
	int32_t half_num_elements = (int32_t) ceilf((float) num_elements / 2.0f);

	// Intilize Array:
		
	float *buffer = NULL;
    //Need error checking:
	cudaMalloc((void**)&buffer, sizeof(float) * half_num_elements); 
    
	// Parrallel reduction:
    	
    if (num_elements == 1)
    {
        _setBufferValue<<<1,1>>>(buffer, array);
    }
    else
    {
        int32_t grid_size = ((half_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
        cudaFindMax_<<<grid_size,BLOCK_SIZE>>>
            (array, buffer, half_num_elements, num_elements);

        num_elements      = half_num_elements;
        half_num_elements /= 2;

        while (num_elements > 1)
        {		
            grid_size = ((half_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
            cudaFindMax_<<<grid_size,BLOCK_SIZE>>>(
                buffer, 
                buffer, 
                half_num_elements, 
                num_elements
            );

            num_elements      = half_num_elements;
            half_num_elements = (int32_t) ceilf((float) half_num_elements / 2.0f);
        }
    }
    
	float max = 0.0f;
	// Copy array back to host:
    CUDA_CALL(
        cudaMemcpy(
            &max, 
            (float*) buffer, 
            sizeof(float), 
            cudaMemcpyDeviceToHost)
        );
	
	cudaFree(buffer);
	return max;
}

__global__ void cudaGetSegmentValuesByIndex(
          float   *array, 
		  float   *output,
    const int32_t  index,
    const int32_t  num_elements_per_segment,
    const int32_t  num_segments
	) {
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, add opposite side of array:
    if (tid < num_segments)
	{
        output[tid] = array[(tid*num_elements_per_segment) + index];
    }    	  
}

__global__ void cudaFindAbsMaxMulti_(
          float   *array, 
		  float   *buffer,
    const int32_t  num_elements_per_segment,
	const int32_t  half_num_elements,
    const int32_t  num_elements,
    const int32_t  total_num_elements
	) {
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    const int32_t segment_index = (int32_t)
        floorf((float)tid / (float)num_elements_per_segment);
            
    const int32_t index = (int32_t)
        tid % num_elements_per_segment;
    
    const int32_t start_index = segment_index * num_elements_per_segment;
    
    const int32_t comparision_index = start_index + num_elements - index;    
    if ((index < half_num_elements) && (comparision_index < total_num_elements))
    {
        const float comparison_value = fabs(array[comparision_index]);

        if (tid < total_num_elements)
        {
            buffer[tid] = 
				(fabs(array[tid]) >= comparison_value) 
				? fabs(array[tid]) : comparison_value;
        }
    }
}

extern "C" float cudaFindAbsMaxMulti(
          float   *array, 
          float   *max_array,
    const int32_t  num_segments,
    const int32_t  num_elements_per_segment,
    const int32_t  total_num_elements
	) {

	int32_t num_elements      = num_elements_per_segment;    
	int32_t half_num_elements = 
        (int32_t) ceilf((float) num_elements_per_segment / 2.0f);

	// Intilize Array:
	float *buffer = NULL;
	CUDA_CALL(
        cudaMalloc((void**)&buffer, sizeof(float) * (size_t)total_num_elements)
    ); 
    
	// Parrallel reduction:
	int32_t grid_size = ((total_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
	cudaFindAbsMaxMulti_<<<grid_size,BLOCK_SIZE>>>(            
        array, 
        buffer, 
        num_elements_per_segment,
        half_num_elements, 
        num_elements,
        total_num_elements
    );
    
	num_elements      = half_num_elements;
	half_num_elements = (int32_t) ceilf((float) half_num_elements / 2.0f);
	
	while (num_elements > 1)
	{		
		cudaFindAbsMaxMulti_<<<grid_size,BLOCK_SIZE>>>(
            buffer, 
            buffer, 
            num_elements_per_segment,
            half_num_elements, 
            num_elements,
            total_num_elements
        );
		
		num_elements      = half_num_elements;
		half_num_elements = (int32_t) ceilf((float) half_num_elements / 2.0f);
	}
        
    grid_size = ((num_segments + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaGetSegmentValuesByIndex<<<grid_size,BLOCK_SIZE>>>(
        buffer, 
        max_array,
        0,
        num_elements_per_segment,
        num_segments
	);
    
	cudaFree(buffer);
    
	return 0;
}

__global__ void cudaSetArgArrayMulti_(
          float   *array, 
    const int32_t  total_num_elements,
    const int32_t  num_elements_per_segment
	) {
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
            
    const int32_t index = (int32_t)
        tid % num_elements_per_segment;
        
    if (tid < total_num_elements)
    {
        array[tid] = (float) index;
    }
}

__global__ void cudaFindArgAbsMaxMulti_(
          float   *array, 
          float   *arg_array,
		  float   *buffer,
          float   *arg_buffer,
    const int32_t  num_elements_per_segment,
	const int32_t  half_num_elements,
    const int32_t  num_elements,
    const int32_t  total_num_elements
	) {
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    const int32_t segment_index = (int32_t)
        floorf((float)tid / (float)num_elements_per_segment);
            
    const int32_t index = (int32_t)
        tid % num_elements_per_segment;
    
    const int32_t start_index = segment_index * num_elements_per_segment;
    
    const int32_t comparision_index = start_index + num_elements - index;    
    if ((index < half_num_elements) && (comparision_index < total_num_elements))
    {
        const float comparison_value = fabs(array[comparision_index]);
        const float comparison_arg = arg_array[comparision_index];

        if (tid < total_num_elements)
        {
            const bool is_bigger = (fabs(array[tid]) >= comparison_value);
        
            buffer[tid] = 
				is_bigger
				? fabs(array[tid]) : comparison_value;
            
            arg_buffer[tid] =
                is_bigger
                ? arg_array[tid] : comparison_arg;
        }
    }
}

extern "C" float cudaFindArgAbsMaxMulti(
          float   *array, 
          float   *arg_max_array,
    const int32_t  num_segments,
    const int32_t  num_elements_per_segment,
    const int32_t  total_num_elements
	) {

	int32_t num_elements      = num_elements_per_segment;    
	int32_t half_num_elements = 
        (int32_t) ceilf((float) num_elements_per_segment / 2.0f);

	// Intilize Array:
	float *value_buffer = NULL;
	CUDA_CALL(
        cudaMalloc((void**)&value_buffer, sizeof(float) * (size_t)total_num_elements)
    ); 
    
    // Intilize Array:
	float *arg_buffer = NULL;
	CUDA_CALL(
        cudaMalloc((void**)&arg_buffer, sizeof(float) * (size_t)total_num_elements)
    ); 
    
    int32_t grid_size = ((total_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaSetArgArrayMulti_<<<grid_size,BLOCK_SIZE>>>(
        arg_buffer, 
        total_num_elements,
        num_elements_per_segment
	);
        
	// Parrallel reduction:
	cudaFindArgAbsMaxMulti_<<<grid_size,BLOCK_SIZE>>>(            
        array, 
        arg_buffer,
        value_buffer, 
        arg_buffer,
        num_elements_per_segment,
        half_num_elements, 
        num_elements,
        total_num_elements
    );
        
	num_elements      = half_num_elements;
	half_num_elements = (int32_t) ceilf((float) half_num_elements / 2.0f);
	
	while (num_elements > 1)
	{		
		cudaFindArgAbsMaxMulti_<<<grid_size,BLOCK_SIZE>>>(
            value_buffer, 
            arg_buffer, 
            value_buffer, 
            arg_buffer, 
            num_elements_per_segment,
            half_num_elements, 
            num_elements,
            total_num_elements
        );
		
		num_elements      = half_num_elements;
		half_num_elements = (int32_t) ceilf((float) half_num_elements / 2.0f);
	}
        
    grid_size = ((num_segments + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaGetSegmentValuesByIndex<<<grid_size,BLOCK_SIZE>>>(
        arg_buffer, 
        arg_max_array,
        0,
        num_elements_per_segment,
        num_segments
	);
        
	cudaFree(value_buffer);
    cudaFree(arg_buffer);
    
	return 0;
}

__global__ void cudaFindAbsMaxMulti16_(
          float16_2_t *array, 
		  float16_2_t *buffer,
    const int32_t      num_elements_per_segment,
	const int32_t      half_num_elements,
    const int32_t      num_elements,
    const int32_t      total_num_elements
	) {
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    const int32_t segment_index = (int32_t)
        floorf((float)tid / (float)num_elements_per_segment);
            
    const int32_t index = (int32_t)
        tid % num_elements_per_segment;
    
    const int32_t start_index = segment_index * num_elements_per_segment;
    
    const int32_t comparision_index = start_index + num_elements - index;    
    if ((index < half_num_elements) && (comparision_index < total_num_elements))
    {
        const float16_2_t comparison_value = __habs2(array[comparision_index]);

        if (tid < total_num_elements)
        {
            buffer[tid] = 
				__hge2 (__habs2(array[tid]), comparison_value)
				* __habs2(array[tid]) 
				+ __hlt2(__habs2(array[tid]), comparison_value)*comparison_value;
        }
    }
}

extern "C" float cudaFindAbsMaxMulti16(
          float16_2_t *array, 
          float16_2_t *max_array,
    const int32_t      num_segments,
    const int32_t      num_elements_per_segment,
    const int32_t      total_num_elements
	) {

	int32_t num_elements      = num_elements_per_segment;    
	int32_t half_num_elements = 
        (int32_t) ceilf((float) num_elements_per_segment / 2.0f);

	// Intilize Array:
	float16_2_t *buffer = NULL;
	CUDA_CALL(
        cudaMalloc((void**)&buffer, sizeof(float16_2_t) * (size_t)total_num_elements)
    ); 
    
	// Parrallel reduction:
	int32_t grid_size = ((total_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
	cudaFindAbsMaxMulti16_<<<grid_size,BLOCK_SIZE>>>(            
        array, 
        buffer, 
        num_elements_per_segment,
        half_num_elements, 
        num_elements,
        total_num_elements
    );
    
	num_elements      = half_num_elements;
	half_num_elements = (int32_t) ceilf((float) half_num_elements / 2.0f);
	
	while (num_elements > 1)
	{		
		cudaFindAbsMaxMulti16_<<<grid_size,BLOCK_SIZE>>>(
            buffer, 
            buffer, 
            num_elements_per_segment,
            half_num_elements, 
            num_elements,
            total_num_elements
        );
		
		num_elements      = half_num_elements;
		half_num_elements = (int32_t) ceilf((float) half_num_elements / 2.0f);
	}
        
    grid_size = ((num_segments + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaGetSegmentValuesByIndex<<<grid_size,BLOCK_SIZE>>>(
        (float*)buffer, 
        (float*)max_array,
        0,
        num_elements_per_segment,
        num_segments
	);
    
	cudaFree(buffer);
    
	return 0;
}

extern "C" int32_t cudaNormaliseArrayMulti(
          float   *array,
    const int32_t  num_segments,
    const int32_t  num_elements_per_segment,
    const int32_t  total_num_elements
    ) {
        
    float *max_array = NULL;
    CUDA_CALL(cudaMalloc((void**)&max_array, sizeof(float)*num_segments));
    
    cudaFindAbsMaxMulti(
        array, 
        max_array,
        num_segments,
        num_elements_per_segment,
        total_num_elements
	); 
    const int32_t grid_size  = ((total_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    
    cudaDivideByValueMulti<<<grid_size,BLOCK_SIZE>>>(
        array, 
        max_array, 
        num_segments,
        num_elements_per_segment,
        total_num_elements
    );
        
    cudaFree(max_array);
    
    return 0;
}

extern "C" int32_t cudaNormaliseArrayMulti16(
          float16_2_t *array,
    const int32_t      num_segments,
    const int32_t      num_elements_per_segment,
    const int32_t      total_num_elements
    ) {
        
    float16_2_t *max_array = NULL;
    CUDA_CALL(cudaMalloc((void**)&max_array, sizeof(float16_2_t)*num_segments));
    
    cudaFindAbsMaxMulti16(
        array, 
        max_array,
        num_segments,
        num_elements_per_segment,
        total_num_elements
	); 
    const int32_t grid_size  = ((total_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
    
    cudaDivideByValueMulti16<<<grid_size,BLOCK_SIZE>>>(
        array, 
        max_array, 
        num_segments,
        num_elements_per_segment,
        total_num_elements
    );
        
    cudaFree(max_array);
    
    return 0;
}

extern "C" int32_t testCudaFindMax(
    const int32_t verbosity
    ) {
    
    /**
      * Test cudaFindMax function.
      * @param 
      *     const int32_t verbosity: Set verbosity of test.
      * @see cudaFindMax()
      * @return int32_t pass or error function.
      */
    
          int32_t pass = true;
	const int32_t test_array_num_elements = 10;
    
    // Allocate host memory:
	const float array[] = 
        {0.0f, 0.1f, 0.2f, 0.1f, 0.0f, -1.0f, (float) M_PI, 1000.0f, 1e9, -1e9};
	
    // Allocate device memory:
    float *array_g;
    CUDA_CALL(cudaMalloc((void**)&array_g, sizeof(float) * test_array_num_elements));
	
	// Transfer data from host to device memory:
    CUDA_CALL(
        cudaMemcpy(
            array_g, 
            array, 
            sizeof(float) * test_array_num_elements, 
            cudaMemcpyHostToDevice)
        );
	
	// Execute kernal:
	float max = cudaFindMaxHost(array_g, test_array_num_elements);		
	
    // Verification:    
    pass *= (fabs(max - 1E9) < MAX_ERR);
    
    // Print result:
    if (verbosity >= 3) 
	{        
        printTestResult(pass, "testCudaFindMax");
    }
    
    // Deallocate device memory:
    cudaFree(array_g);
    
    return !pass;
}

__global__ void cudaFindAbsMax_(
          float   *array , 
		  float   *buffer,
	const int32_t  half_num_elements,
    const int32_t  num_elements
	) {
    
    // Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, add opposite side of array:
    if (tid < half_num_elements) 
	{
        buffer[tid] = 
              (fabs(array[tid]) >= fabs(array[num_elements - 1 - tid])) 
            ? abs(array[tid]) : abs(array[num_elements - 1 - tid]); 
    }    	
}

extern "C" float cudaFindAbsMax(
          float   *array , 
    const int32_t  num_elements_o
	) {

	int32_t num_elements      = num_elements_o;
	int32_t half_num_elements = (int32_t) ceilf((float) num_elements / 2.0f);

	// Intilize Array:
    float *buffer;
    
    // Need error checking 
    cudaMalloc((void**)&buffer, sizeof(float) * half_num_elements); 
    
	// Parrallel reduction
	int32_t grid_size = ((half_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
	cudaFindAbsMax_<<<grid_size,BLOCK_SIZE>>>(
        array, 
        buffer,
        half_num_elements, 
        num_elements
    );

	num_elements      = half_num_elements;
	half_num_elements /= 2;
	
	while (num_elements > 1)
	{		
		grid_size = ((half_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);
		cudaFindAbsMax_<<<grid_size,BLOCK_SIZE>>>(
            buffer, 
            buffer, 
            half_num_elements, 
            num_elements
        );
		
		num_elements      = half_num_elements;
		half_num_elements = (int32_t) ceilf((float) half_num_elements / 2.0f);
	}
	
	float max = 0.0;
	// Copy array back to host:
    CUDA_CALL(
        cudaMemcpy(
            &max, 
            (float*) buffer, 
            sizeof(float), 
            cudaMemcpyDeviceToHost
        )
    );
	
	cudaFree(buffer);
	return max;
}

extern "C" int32_t testCudaFindAbsMax(
    const int32_t verbosity
    ) {
    
    /**
      * Test cudaFindAbsMax function.
      * @param 
      *     const int32_t verbosity: Set verbosity of test.
      * @see cudaFindAbsMax()
      * @return int32_t pass or error function.
      */
    
          int32_t pass = true;
	const int32_t test_array_num_elements = 10;
    
    // Allocate host memory:
	const float array[] = 
        {0.0f, 0.1f, 0.2f, 0.1f, 0.0f, -1.0f, (float)M_PI, 1000.0f, 1e9, -1e10};
	
    // Allocate device memory:
    float *array_g;
    CUDA_CALL(cudaMalloc((void**)&array_g, sizeof(float) * test_array_num_elements));
	
	// Transfer data from host to device memory:
    CUDA_CALL(
        cudaMemcpy(
            array_g, 
            array, 
            sizeof(float) * test_array_num_elements, 
            cudaMemcpyHostToDevice)
        );
	
	// Execute kernal:
	float max = cudaFindAbsMax(array_g , test_array_num_elements);		

    // Verification:    
    pass *= (fabs(max - 1E10) < MAX_ERR);
    
    // Print result:
    if (verbosity >= 3) 
	{    
        printTestResult(pass, "testCudaFindMaxAbs");
    }
    
    // Deallocate device memory:
    cudaFree(array_g);
    
    return !pass;
}

extern "C" void cudaResample(
	const int32_t         old_num_elements,
	//const int32_t         new_num_elements,
	const int32_t         max_num_elements,
		  cuFloatComplex *window, 
		  cuFloatComplex *array
	) {
	
	const int32_t num_frequency_samples = old_num_elements/2 + 1;

	//Peform RFFT:
	cudaRfft(old_num_elements, 1, array);

	//Set excess array values to zero:
    const int32_t padding_num_elements = max_num_elements - num_frequency_samples;
          int32_t grid_size = ((padding_num_elements + BLOCK_SIZE - 1) / BLOCK_SIZE);

    cudaSetArrayValueComplex<<<grid_size,BLOCK_SIZE>>>(
        &array[num_frequency_samples], 
        (cuFloatComplex) {0.0f, 0.0f}, 
        padding_num_elements
    );

	// Muiltiply by window:
    grid_size = ((num_frequency_samples + BLOCK_SIZE - 1) / BLOCK_SIZE);
    cudaMultiplyC<<<grid_size,BLOCK_SIZE>>>(
        array, 
        window, 
        num_frequency_samples
    );
    
    printf("Needs fixing! i dont think this is how you resample anyway\n");
    exit(1);
	// Perfrom IRFFT:
	//cudaIRfft(new_num_elements, 1, new_num_elements, array);
}

extern "C" __global__ void cudaRealF(
	      cuFloatComplex *complex_array, 
	      float          *real_array   ,
	const int32_t         num_elements
	) {
	
	// Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, add opposite side of array:
    if (tid < num_elements) 
	{
        real_array[tid] = complex_array[tid].x;	
	}
}

extern "C" __global__ void cudaComplexF(
	      float          *real_array   ,
	      cuFloatComplex *complex_array, 
	const int32_t         num_elements
	) {
	
	// Assign unique thread index for GPU calculation:
    const int32_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // If not outside array bounds, add opposite side of array:
    if (tid < num_elements) 
	{
        complex_array[tid].x = real_array[tid];	
		complex_array[tid].y = 0.0f;	
	}
}

#endif

/*
if domain not in ('time', 'freq'):
        raise ValueError("Acceptable domain flags are 'time' or"
                         " 'freq', not domain={}".format(domain))

    x = np.asarray(x)
    Nx = x.shape[axis]

    # Check if we can use faster real FFT
    real_input = np.isrealobj(x)

    if domain == 'time':
        # Forward transform
        if real_input:
            X = sp_fft.rfft(x, axis=axis)
        else:  # Full complex FFT
            X = sp_fft.fft(x, axis=axis)
    else:  # domain == 'freq'
        X = x

    # Apply window to spectrum
    if window is not None:
        if callable(window):
            W = window(sp_fft.fftfreq(Nx))
        elif isinstance(window, np.ndarray):
            if window.shape != (Nx,):
                raise ValueError('window must have the same num_elements as data')
            W = window
        else:
            W = sp_fft.ifftshift(get_window(window, Nx))

        newshape_W = [1] * x.ndim
        newshape_W[axis] = X.shape[axis]
        if real_input:
            # Fold the window back on itself to mimic complex behavior
            W_real = W.copy()
            W_real[1:] += W_real[-1:0:-1]
            W_real[1:] *= 0.5
            X *= W_real[:newshape_W[axis]].reshape(newshape_W)
        else:
            X *= W.reshape(newshape_W)

    # Copy each half of the original spectrum to the output spectrum, either
    # truncating high frequences (downsampling) or zero-padding them
    # (upsampling)

    # Placeholder array for output spectrum
    newshape = list(x.shape)
    if real_input:
        newshape[axis] = num // 2 + 1
    else:
        newshape[axis] = num
    Y = np.zeros(newshape, X.dtype)

    # Copy positive frequency components (and Nyquist, if present)
    N = min(num, Nx)
    nyq = N // 2 + 1  # Slice index that includes Nyquist if present
    sl = [slice(None)] * x.ndim
    sl[axis] = slice(0, nyq)
    Y[tuple(sl)] = X[tuple(sl)]
    if not real_input:
        # Copy negative frequency components
        if N > 2:  # (slice expression doesn't collapse to empty array)
            sl[axis] = slice(nyq - N, None)
            Y[tuple(sl)] = X[tuple(sl)]

    # Split/join Nyquist component(s) if present
    # So far we have set Y[+N/2]=X[+N/2]
    if N % 2 == 0:
        if num < Nx:  # downsampling
            if real_input:
                sl[axis] = slice(N//2, N//2 + 1)
                Y[tuple(sl)] *= 2.
            else:
                # select the component of Y at frequency +N/2,
                # add the component of X at -N/2
                sl[axis] = slice(-N//2, -N//2 + 1)
                Y[tuple(sl)] += X[tuple(sl)]
        elif Nx < num:  # upsampling
            # select the component at frequency +N/2 and halve it
            sl[axis] = slice(N//2, N//2 + 1)
            Y[tuple(sl)] *= 0.5
            if not real_input:
                temp = Y[tuple(sl)]
                # set the component at -N/2 equal to the component at +N/2
                sl[axis] = slice(num-N//2, num-N//2 + 1)
                Y[tuple(sl)] = temp

    # Inverse transform
    if real_input:
        y = sp_fft.irfft(Y, num, axis=axis)
    else:
        y = sp_fft.ifft(Y, axis=axis, overwrite_x=True)

    y *= (float(num) / float(Nx))

    if t is None:
        return y
    else:
        new_t = np.arange(0, num) * (t[1] - t[0]) * Nx / float(num) + t[0]
        return y, new_t
        
*/