{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb362f8",
   "metadata": {},
   "source": [
    "# Setting up the Environment\n",
    "\n",
    "Gravflow is a GPU based module, it relies heavily on vectorised GPU functions in order to speed up calculations, and many parts of it (primarily cuPhenom), do not currently have a CPU alternative. Most of the functions are built from TensorFlow functions, which can be run in CPU only operation, albeit not as efficiently, and doing so will loose much of the advantage this module provides over other alternative. You're welcome to try to run GravyFlow in a CPU only environment, but I would not count on it.\n",
    "\n",
    "Much of the enironment setup can be handled automatically by running the `setup.sh` bash script in the base directory, please note that GravyFlow is a very young module early in its development and there are likely to be teething problems, especially with the automated helper scripts. If the setup script does not work, try to ensure that you are running GravyFlow in an enviroment in which TensorFlow can see avalible GPUs.\n",
    "\n",
    "## Sharing is caring\n",
    "\n",
    "Presumably, you will be running this on a GPU compute cluster which you share with other people. This means that you have to select which GPU(s) you are going to run on before you execute any programs. This first notebook will teach you how to ensure GravyFlow automatically selects a GPU with avalible GPU memory, and ensures your enviroment is set up in a way to run GravyFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf67322",
   "metadata": {},
   "source": [
    "## Notebook imports\n",
    "\n",
    "First we will import several built-in (os, sys) packages, and one depedancy, TensorFlow. These will be used by some blocks in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd8239",
   "metadata": {},
   "source": [
    "## Import GravyFlow\n",
    "\n",
    "Next we will import the Gravyflow module. Currenly GravyFlow is not setup to be installable, so to ensure that this notebook can import it, a relative path from the grandfather directory of this notebook is used. If you have moved this notebook, this will need to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feccc33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-02 13:59:39.267249: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-02 13:59:40.266379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael.norman/data_ad_infinitum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael.norman/.conda/envs/gravyflow/lib/python3.10/site-packages/gwpy/time/__init__.py:36: UserWarning: Wswiglal-redir-stdio:\n",
      "\n",
      "SWIGLAL standard output/error redirection is enabled in IPython.\n",
      "This may lead to performance penalties. To disable locally, use:\n",
      "\n",
      "with lal.no_swig_redirect_standard_output_error():\n",
      "    ...\n",
      "\n",
      "To disable globally, use:\n",
      "\n",
      "lal.swig_redirect_standard_output_error(True)\n",
      "\n",
      "Note however that this will likely lead to error messages from\n",
      "LAL functions being either misdirected or lost when called from\n",
      "Jupyter notebooks.\n",
      "\n",
      "To suppress this warning, use:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
      "import lal\n",
      "\n",
      "  from lal import LIGOTimeGPS\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path of the parent directory\n",
    "parent_dir = os.path.abspath('../../')\n",
    "print(parent_dir)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# To import gravyflow simply use:\n",
    "import gravyflow as gf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c64bc5",
   "metadata": {},
   "source": [
    "# GPU Setup\n",
    "\n",
    "For ease of use, GravyFlow comes with a function to automatically sort out the GPU environment: `gf.env`.\n",
    "\n",
    "It has the following optional arguments:\n",
    "\n",
    "- `min_gpu_memory_mb`: int = 4000\n",
    "  - This sets the minimum required GPU memory in megabytes for a GPU to be considered as free and allocated for use by GravyFlow. By default, this is 4000 Mb.\n",
    "- `num_gpus_to_request`: int = 1\n",
    "  - This determines the number of GPUs that GravyFlow will attempt to find and subsequently run on. The default is 1.\n",
    "- `memory_to_allocate_tf`: int = 2000\n",
    "  - This determines the amount of GPU memory that GravyFlow will allocate per GPU. This value is fixed, rather than allowing growth, to ensure that CUDA functions run by GravyFlow have memory to run. If you are using cuPhenom for CBC generation, ensure that there is room left on the GPU for it to run when you set this value. It defaults to 2000 Mb.\n",
    "\n",
    "## Return Object\n",
    "\n",
    "The function returns the following object:\n",
    "\n",
    "- `strategy`: tf.distribute.Strategy\n",
    "  - This object will allow for multi-GPU usage in subsequent TensorFlow operations.\n",
    "\n",
    "## Function Operations\n",
    "\n",
    "Calling this function will do a number of things:\n",
    "\n",
    "1. Determine avalible GPUs which have free GPU memory greater than the requested: `min_gpu_memory_mb`.\n",
    "2. Allocate a number of free GPUs equivalent to `num_gpus_to_request` for GravyFlow (TensorFlow) and cuPhenom (CUDA), by setting the `CUDA_VISIBLE_DEVICES` environmental variable.\n",
    "3. Attempt to check that your CUDA version is compatible with your TensorFlow version.\n",
    "4. Allocate `memory_to_allocate_tf` Mb of GPU memory to GravyFlow (TensorFlow) per requested GPU.\n",
    "5. Setup and return a tf.distribute.Strategy object which will allow for multi-GPU usage in subsequent TensorFlow operations.\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "Below is an example of its operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7298c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Setup enviroment and return tf.distributed stratergy object.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[39m=\u001b[39m gf\u001b[39m.\u001b[39menv()\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m env:\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     \u001b[39m# ALl code placed her will be in the scope of the TensorFlow strategy.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[39m# Print CUDA_VISIBLE_DEVICES after gf.env() has run:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     cuda_visible_devices \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNot set\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gf' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup enviroment and return tf.distributed stratergy object.\n",
    "env = gf.env()\n",
    "\n",
    "with env:\n",
    "\n",
    "    # ALl code placed her will be in the scope of the TensorFlow strategy.\n",
    "\n",
    "    # Print CUDA_VISIBLE_DEVICES after gf.env() has run:\n",
    "    cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')\n",
    "    print(f'CUDA_VISIBLE_DEVICES after environment setup: {cuda_visible_devices}')\n",
    "\n",
    "    # Print devices visible to TensorFlow after gf.env() has run:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"GPUs visible to TensorFlow after environment setup: {gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd5057",
   "metadata": {},
   "source": [
    "## Important Notes on using gf.env:\n",
    "- Running any TensorFlow functionality before initilising the environment will cause it to run on all GPUs by default.\n",
    "- You cannot setup the environment more than once per Python kernel.\n",
    "\n",
    "Now that we have seen how we can automatically setup the environment, let us move of to acquiring our first interferometer noise background dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('gravyflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6f1be94f56c97543b1174f0691434140ad3c33189a0dd1833b365759a987259"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
