{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb362f8",
   "metadata": {},
   "source": [
    "# Setting up the Environment\n",
    "\n",
    "Gravflow is a GPU based module, it relies heavily on vectorised GPU functions in order to speed up calculations, and many parts of it (primarily cuPhenom), do not currently have a CPU alternative. Most of the functions are built from TensorFlow functions, which can be run in CPU only operation, albeit not as efficiently, and doing so will loose much of the advantage this module provides over other alternative. You're welcome to try to run GravyFlow in a CPU only environment, but it has not been rigerously tested.\n",
    "\n",
    "Much of the enironment setup can be handled automatically by running the `setup.sh` bash script in the base directory. Note that GravyFlow is still in development and there are likely to be teething problems, especially with the automated helper scripts. If the setup script does not work, try to ensure that you are running GravyFlow in an enviroment in which TensorFlow can see avalible GPUs.\n",
    "\n",
    "## Sharing is caring\n",
    "\n",
    "Presumably, you will be running this on a GPU compute cluster which you share with other people. This means that you have to select which GPU(s) you are going to run on before you execute any programs. This first notebook will teach you how to ensure GravyFlow automatically selects a GPU with avalible GPU memory, and ensures your enviroment is set up in a way to run GravyFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf67322",
   "metadata": {},
   "source": [
    "## Notebook imports\n",
    "\n",
    "First we will import several built-in (os, sys) packages, and one depedancy, TensorFlow. These will be used by some blocks in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b3d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 01:51:07.950829: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-09 01:51:16.765879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd8239",
   "metadata": {},
   "source": [
    "## Import GravyFlow\n",
    "\n",
    "Next we will import the Gravyflow module. Currenly GravyFlow is not setup to be installable, so to ensure that this notebook can import it, a relative path from the grandfather directory of this notebook is used. If you have moved this notebook, this will need to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feccc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path of the parent directory\n",
    "parent_dir = os.path.abspath('../../')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# To import gravyflow simply use:\n",
    "import gravyflow as gf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c64bc5",
   "metadata": {},
   "source": [
    "## GPU Setup\n",
    "\n",
    "For ease of use, GravyFlow comes with a function to automatically sort out the GPU environment: `gf.env`.\n",
    "\n",
    "It has the following optional arguments:\n",
    "\n",
    "- `min_gpu_memory_mb` : int = 4000\n",
    "  >This sets the minimum required GPU memory in megabytes for a GPU to be considered as free and allocated for use by GravyFlow. By default, this is 4000 Mb.\n",
    "- `max_gpu_utilization_percentage` : float = 80\n",
    "  > The maximum GPU utalisation a GPU can have as a percetentage, before GravyFlow disallows utalization. Default is 70.\n",
    "- `num_gpus_to_request` : int = 1\n",
    "  > This determines the number of GPUs that GravyFlow will attempt to find and subsequently run on. The default is 1.\n",
    "- `memory_to_allocate_tf` : int = 2000\n",
    "  > This determines the amount of GPU memory that GravyFlow will allocate per GPU. This value is fixed, rather than allowing growth, to ensure that CUDA functions run by GravyFlow have memory to run. If you are using cuPhenom for CBC generation, ensure that there is room left on the GPU for it to run when you set this value. It defaults to 2000 Mb. \n",
    "- `gpus` : Union[str, int, List[Union[int, str]], None] = None\n",
    "  > If this variable is set, it will force allocation on inputted GPUs. This is not reccomended unless you have a specific reason to run on specific GPUs. If set to None, which is default, GravyFlow will automatically allocate a free GPU(s).\n",
    "\n",
    "The function returns the following object:\n",
    "\n",
    "- `strategy` : tf.distribute.Strategy\n",
    "  > This object will allow for multi-GPU usage in subsequent TensorFlow operations.\n",
    "\n",
    "## Function Operations\n",
    "\n",
    "Calling this function will do a number of things:\n",
    "\n",
    "1. Determine avalible GPUs which have free GPU memory greater than the requested: `min_gpu_memory_mb`.\n",
    "2. Allocate a number of free GPUs equivalent to `num_gpus_to_request` for GravyFlow (TensorFlow) and cuPhenom (CUDA), by setting the `CUDA_VISIBLE_DEVICES` environmental variable.\n",
    "3. Attempt to check that your CUDA version is compatible with your TensorFlow version.\n",
    "4. Allocate `memory_to_allocate_tf` Mb of GPU memory to GravyFlow (TensorFlow) per requested GPU.\n",
    "5. Setup and return a tf.distribute.Strategy object which will allow for multi-GPU usage in subsequent TensorFlow operations.\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "Below is an example of its operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db7298c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:TensorFlow version: 2.12.1, CUDA version: 11.8\n",
      "2024-01-09 01:53:39.806029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2000 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "INFO:root:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES after environment setup: 2\n",
      "GPUs visible to TensorFlow after environment setup: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Setup enviroment and return tf.distributed stratergy object.\n",
    "env = gf.env()\n",
    "\n",
    "with env:\n",
    "\n",
    "    # ALl code placed here will be in the scope of the TensorFlow strategy.\n",
    "\n",
    "    # Print CUDA_VISIBLE_DEVICES after gf.env() has run:\n",
    "    cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set.')\n",
    "    print(f'CUDA_VISIBLE_DEVICES after environment setup: {cuda_visible_devices}')\n",
    "\n",
    "    # Print devices visible to TensorFlow after gf.env() has run:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"GPUs visible to TensorFlow after environment setup: {gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd5057",
   "metadata": {},
   "source": [
    "## Important Notes on using gf.env:\n",
    "- Running any TensorFlow functionality before initilising the environment will cause it to run on all GPUs by default.\n",
    "- You cannot setup the environment more than once per Python kernel.\n",
    "\n",
    "Now that we have seen how we can automatically setup the environment, let us move of to acquiring our first interferometer noise background dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccbe5cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:TensorFlow version: 2.12.1, CUDA version: 11.8\n",
      "INFO:root:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES after environment setup: 2,4. This has changed correctly.\n",
      "GPUs visible to TensorFlow after environment setup: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]. This has not changed.\n"
     ]
    }
   ],
   "source": [
    "# If we attempt to run gf.env again, this time with 2 GPUs requested, without restarting the kernel.\n",
    "env = gf.env(num_gpus_to_request=2)\n",
    "\n",
    "# gf.env will prevent a new scope from being created, as this is not allowed by TensorFlow, and simply\n",
    "# return the same enviroment.\n",
    "with env:\n",
    "    cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set.')\n",
    "    print(f'CUDA_VISIBLE_DEVICES after environment setup: {cuda_visible_devices}. This has changed correctly.')\n",
    "\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"GPUs visible to TensorFlow after environment setup: {gpus}. This has not changed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('gravyflow-212')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c749986c5f458fb2b067b85dd36de77f848d054cfc4dbd65eb5821cf27edab2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
