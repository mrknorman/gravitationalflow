{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model with a GravyFlow Dataset\n",
    "\n",
    "In this notebook we will use our generated dataset to train a keras model. We start with the needed imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in imports\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "# Dependency imports: \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Flatten, Dropout, ELU\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Import the GravyFlow module.\n",
    "import gravyflow as gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment using gf.env() and return a tf.distribute.Strategy object.\n",
    "env : tf.distribute.Strategy = gf.env(\n",
    "\tmemory_to_allocate_tf=8000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a TensorFlow dataset through composition:\n",
    "\n",
    "Rather than generating a generic Python iterator, we can also use GravyFlow to create a custom TensorFlow dataset. This will give us the ability to utalise all the functionality provided by the TensorFlow dataset class, including seamless integration with keras models, whilst maintaining the ability to generate datasets quickly enough for real time training, only caching downloaded data segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a keras model, inspired by a model from the literature, found at Gabbard _et at._ here: https://link.aps.org/doi/10.1103/PhysRevLett.120.141103:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gabbard(\n",
    "        input_shape_onsource : int, \n",
    "        input_shape_offsource : int\n",
    "    ) -> tf.keras.Model:\n",
    "    \n",
    "    # Define the inputs based on the dictionary keys and expected shapes\n",
    "    # Replace `input_shape_onsource` and `input_shape_offsource` with the actual shapes\n",
    "    onsource_input = Input(shape=input_shape_onsource, name=\"ONSOURCE\")\n",
    "    offsource_input = Input(shape=input_shape_offsource, name=\"OFFSOURCE\")\n",
    "\n",
    "    # Pass the inputs to your custom Whiten layer\n",
    "    # Assuming your Whiten layer can handle multiple inputs\n",
    "    whiten_output = gf.Whiten()([onsource_input, offsource_input])\n",
    "\n",
    "    x = gf.Reshape()(whiten_output)\n",
    "    \n",
    "    # Convolutional and Pooling layers\n",
    "    x = Conv1D(8, 64, padding='same', name=\"Convolutional_1\")(x)\n",
    "    x = ELU(name=\"ELU_1\")(x)\n",
    "    x = MaxPooling1D(pool_size=4, strides=4, name=\"Pooling_1\", padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(8, 32, padding='same', name=\"Convolutional_2\")(x)\n",
    "    x = ELU(name=\"ELU_2\")(x)\n",
    "    x = Conv1D(16, 32, padding='same', name=\"Convolutional_3\")(x)\n",
    "    x = ELU(name=\"ELU_3\")(x)\n",
    "    x = MaxPooling1D(pool_size=4, strides=4, name=\"Pooling_3\", padding=\"same\")(x)\n",
    "    \n",
    "    # Flatten layer\n",
    "    x = Flatten(name=\"Flatten\")(x)\n",
    "    \n",
    "    # Dense layers with dropout\n",
    "    x = Dense(64, name=\"Dense_1\")(x)\n",
    "    x = ELU(name=\"ELU_7\")(x)\n",
    "    x = Dropout(0.5, name=\"Dropout_1\")(x)\n",
    "    \n",
    "    x = Dense(64, name=\"Dense_2\")(x)\n",
    "    x = ELU(name=\"ELU_8\")(x)\n",
    "    x = Dropout(0.5, name=\"Dropout_2\")(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid', name=\"INJECTION_MASKS\")(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[onsource_input, offsource_input], outputs=outputs, name=\"custom\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are only using one injection, we expect our output label to be a single value for each example, therefore we must adjust the dimensionality of the injection masks output with tensorflow datasets mapping functionality, we define the function we want to map to the dataset here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_features(features, labels):\n",
    "    labels['INJECTION_MASKS'] = labels['INJECTION_MASKS'][0]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow and keras requires that the model and training dataset are created in the same scope, and is quite strict about these limitations. Thus we will here create our dataset and our model in the same scope. Nominally, it is anticipated that GravyFlow will mostly be used in Python scripts, rather than notebooks, where this will not be a problem if everything is kept in the same TensorFlow strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gf.env():\n",
    "    # This object will be used to obtain real interferometer data based on specified parameters.\n",
    "    ifo_data_obtainer : gf.IFODataObtainer = gf.IFODataObtainer(\n",
    "        observing_runs=gf.ObservingRun.O3, # Specify the observing run (e.g., O3).\n",
    "        data_quality=gf.DataQuality.BEST,  # Choose the quality of the data (e.g., BEST).\n",
    "        data_labels=[                      # Define the types of data to include.\n",
    "            gf.DataLabel.NOISE, \n",
    "            gf.DataLabel.GLITCHES\n",
    "        ],\n",
    "        segment_order=gf.SegmentOrder.RANDOM, # Order of segment retrieval (e.g., RANDOM).\n",
    "        force_acquisition=True,               # Force the acquisition of new data.\n",
    "        cache_segments=False                  # Choose not to cache the segments.\n",
    "    )\n",
    "\n",
    "    # Initialize the noise generator wrapper:\n",
    "    # This wrapper will use the ifo_data_obtainer to generate real noise based on the specified parameters.\n",
    "    noise: gf.NoiseObtainer = gf.NoiseObtainer(\n",
    "        ifo_data_obtainer=ifo_data_obtainer, # Use the previously set up IFODataObtainer object.\n",
    "        noise_type=gf.NoiseType.REAL,        # Specify the type of noise as REAL.\n",
    "        ifos=gf.IFO.L1                       # Specify the interferometer (e.g., LIGO Livingston L1).\n",
    "    )\n",
    "\n",
    "    scaling_method : gf.ScalingMethod = gf.ScalingMethod(\n",
    "        value=gf.Distribution(\n",
    "            min_=8.0,\n",
    "            max_=15.0,\n",
    "            type_=gf.DistributionType.UNIFORM\n",
    "        ),\n",
    "        type_=gf.ScalingTypes.SNR\n",
    "    )\n",
    "\n",
    "    # Define a uniform distribution for the mass of the first object in solar masses.\n",
    "    mass_1_distribution_msun : gf.Distribution = gf.Distribution(\n",
    "        min_=10.0, \n",
    "        max_=60.0, \n",
    "        type_=gf.DistributionType.UNIFORM\n",
    "    )\n",
    "\n",
    "    # Define a uniform distribution for the mass of the second object in solar masses.\n",
    "    mass_2_distribution_msun : gf.Distribution = gf.Distribution(\n",
    "        min_=10.0, \n",
    "        max_=60.0, \n",
    "        type_=gf.DistributionType.UNIFORM\n",
    "    )\n",
    "\n",
    "    # Define a uniform distribution for the inclination of the binary system in radians.\n",
    "    inclination_distribution_radians : gf.Distribution = gf.Distribution(\n",
    "        min_=0.0, \n",
    "        max_=np.pi, \n",
    "        type_=gf.DistributionType.UNIFORM\n",
    "    )\n",
    "\n",
    "    # Initialize a PhenomD waveform generator with the defined distributions.\n",
    "    # This generator will produce waveforms with randomly varied masses and inclination angles.\n",
    "    phenom_d_generator : gf.WaveformGenerator = gf.cuPhenomDGenerator(\n",
    "        mass_1_msun=mass_1_distribution_msun,\n",
    "        mass_2_msun=mass_2_distribution_msun,\n",
    "        inclination_radians=inclination_distribution_radians,\n",
    "        scaling_method=scaling_method,\n",
    "        injection_chance=0.5 # Set so half produced examples will not contain this signal\n",
    "    )\n",
    "    \n",
    "    training_dataset : tf.data.Dataset = gf.Dataset(       \n",
    "        noise_obtainer=noise,\n",
    "        waveform_generators=phenom_d_generator,\n",
    "        input_variables=[\n",
    "            gf.ReturnVariables.ONSOURCE, \n",
    "            gf.ReturnVariables.OFFSOURCE, \n",
    "        ],\n",
    "        output_variables=[\n",
    "            gf.ReturnVariables.INJECTION_MASKS\n",
    "        ]\n",
    "    ).map(adjust_features)\n",
    "\n",
    "    validation_dataset : tf.data.Dataset = gf.Dataset(       \n",
    "        noise_obtainer=noise,\n",
    "        waveform_generators=phenom_d_generator,\n",
    "        seed=1001, # Implement different seed to generate different waveforms,\n",
    "        group=\"validate\", # Ensure noise is pulled from those marked for validation.\n",
    "        input_variables=[\n",
    "            gf.ReturnVariables.ONSOURCE, \n",
    "            gf.ReturnVariables.OFFSOURCE, \n",
    "        ],\n",
    "        output_variables=[\n",
    "            gf.ReturnVariables.INJECTION_MASKS\n",
    "        ]\n",
    "    ).map(adjust_features)\n",
    "\n",
    "    testing_dataset : tf.data.Dataset = gf.Dataset(       \n",
    "        noise_obtainer=noise,\n",
    "        waveform_generators=phenom_d_generator,\n",
    "        seed=1002, # Implement different seed to generate different waveforms,\n",
    "        group=\"test\", # Ensure noise is pulled from those marked for validation.\n",
    "        input_variables=[\n",
    "            gf.ReturnVariables.ONSOURCE, \n",
    "            gf.ReturnVariables.OFFSOURCE, \n",
    "        ],\n",
    "        output_variables=[\n",
    "            gf.ReturnVariables.INJECTION_MASKS\n",
    "        ]\n",
    "    ).map(adjust_features)\n",
    "    \n",
    "    for input_example, _ in training_dataset.take(1):\n",
    "        input_shape_onsource = input_example[\"ONSOURCE\"].shape[1:]  # Exclude batch dimension    \n",
    "        input_shape_offsource = input_example[\"OFFSOURCE\"].shape[1:] \n",
    "    \n",
    "    model = create_gabbard(input_shape_onsource, input_shape_offsource)\n",
    "\n",
    "    # Now you can print the model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Model compilation\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',  # Or any other loss function appropriate for your task\n",
    "        metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell failed it is most likely because you attempted to run it twice within the same kernel session. The kernal must be restarted in order to generate a fresh TensorFlow stratergy and recompile the model.\n",
    "\n",
    "Finally, we can train the model with our generated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_epoch : int = int(1E5)\n",
    "num_validation_examples : int = int(1E4)\n",
    "num_testing_examples : int = int(1E4)\n",
    "with gf.env(memory_to_allocate_tf=4000): \n",
    "    history = model.fit(\n",
    "        training_dataset,\n",
    "        epochs=10,  # Number of epochs to train for\n",
    "        steps_per_epoch=examples_per_epoch // gf.Defaults.num_examples_per_batch,\n",
    "        validation_data=validation_dataset,\n",
    "        validation_steps=num_validation_examples // gf.Defaults.num_examples_per_batch #Ensure this is set as dataset is uncapped\n",
    "    )\n",
    "\n",
    "    model.evaluate(\n",
    "        testing_dataset, \n",
    "        verbose=2,\n",
    "        steps=num_testing_examples // gf.Defaults.num_examples_per_batch\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gravyflow2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
