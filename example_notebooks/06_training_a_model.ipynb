{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model with a GravyFlow Dataset\n",
    "\n",
    "In this notebook we will use our generated dataset to train a keras model. We start with the needed imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 06:52:36.136120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 06:52:36.136177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 06:52:36.137843: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 06:52:36.147090: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 06:52:37.656815: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Built-in imports\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "# Dependency imports: \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, Flatten, Dropout, ELU\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Import the GravyFlow module.\n",
    "import gravyflow as gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:TensorFlow version: 2.15.0, CUDA version: 12.2\n",
      "2024-03-04 06:52:45.413739: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-03-04 06:52:45.414317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3000 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "INFO:root:[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Set up the environment using gf.env() and return a tf.distribute.Strategy object.\n",
    "env : tf.distribute.Strategy = gf.env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a TensorFlow dataset through composition:\n",
    "\n",
    "Rather than generating a generic Python iterator, we can also use GravyFlow to create a custom TensorFlow dataset. This will give us the ability to utalise all the functionality provided by the TensorFlow dataset class, including seamless integration with keras models, whilst maintaining the ability to generate datasets quickly enough for real time training, only caching downloaded data segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a keras model, inspired by a model from the literature, found at Gabbard _et at._ here: https://link.aps.org/doi/10.1103/PhysRevLett.120.141103:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gabbard(\n",
    "        input_shape_onsource : int, \n",
    "        input_shape_offsource : int\n",
    "    ) -> tf.keras.Model:\n",
    "    \n",
    "    # Define the inputs based on the dictionary keys and expected shapes\n",
    "    # Replace `input_shape_onsource` and `input_shape_offsource` with the actual shapes\n",
    "    onsource_input = Input(shape=input_shape_onsource, name=\"ONSOURCE\")\n",
    "    offsource_input = Input(shape=input_shape_offsource, name=\"OFFSOURCE\")\n",
    "\n",
    "    # Pass the inputs to your custom Whiten layer\n",
    "    # Assuming your Whiten layer can handle multiple inputs\n",
    "    whiten_output = gf.Whiten()([onsource_input, offsource_input])\n",
    "\n",
    "    x = gf.Reshape()(whiten_output)\n",
    "    \n",
    "    # Convolutional and Pooling layers\n",
    "    x = Conv1D(8, 64, padding='same', name=\"Convolutional_1\")(x)\n",
    "    x = ELU(name=\"ELU_1\")(x)\n",
    "    x = MaxPooling1D(pool_size=4, strides=4, name=\"Pooling_1\", padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(8, 32, padding='same', name=\"Convolutional_2\")(x)\n",
    "    x = ELU(name=\"ELU_2\")(x)\n",
    "    x = Conv1D(16, 32, padding='same', name=\"Convolutional_3\")(x)\n",
    "    x = ELU(name=\"ELU_3\")(x)\n",
    "    x = MaxPooling1D(pool_size=4, strides=4, name=\"Pooling_3\", padding=\"same\")(x)\n",
    "    \n",
    "    # Flatten layer\n",
    "    x = Flatten(name=\"Flatten\")(x)\n",
    "    \n",
    "    # Dense layers with dropout\n",
    "    x = Dense(64, name=\"Dense_1\")(x)\n",
    "    x = ELU(name=\"ELU_7\")(x)\n",
    "    x = Dropout(0.5, name=\"Dropout_1\")(x)\n",
    "    \n",
    "    x = Dense(64, name=\"Dense_2\")(x)\n",
    "    x = ELU(name=\"ELU_8\")(x)\n",
    "    x = Dropout(0.5, name=\"Dropout_2\")(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid', name=\"INJECTION_MASKS\")(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[onsource_input, offsource_input], outputs=outputs, name=\"custom\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are only using one injection, we expect our output label to be a single value for each example, therefore we must adjust the dimensionality of the injection masks output with tensorflow datasets mapping functionality, we define the function we want to map to the dataset here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_features(features, labels):\n",
    "    labels['INJECTION_MASKS'] = labels['INJECTION_MASKS'][0]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow and keras requires that the model and training dataset are created in the same scope, and is quite strict about these limitations. Thus we will here create our dataset and our model in the same scope. Nominally, it is anticipated that GravyFlow will mostly be used in Python scripts, rather than notebooks, where this will not be a problem if everything is kept in the same TensorFlow strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 06:52:46.355673: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f305c034970 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-04 06:52:46.355724: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2024-03-04 06:52:46.362331: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-04 06:52:46.414534: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-03-04 06:52:46.474003: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709563966.552056 4060811 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/home/michael.norman/miniconda3/envs/gravyflow/lib/python3.10/site-packages/igwn_auth_utils/x509.py:98: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to not_valid_after_utc.\n",
      "  return (cert.not_valid_after - datetime.utcnow()).total_seconds()\n",
      "/home/michael.norman/miniconda3/envs/gravyflow/lib/python3.10/site-packages/igwn_auth_utils/x509.py:98: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to not_valid_after_utc.\n",
      "  return (cert.not_valid_after - datetime.utcnow()).total_seconds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " ONSOURCE (InputLayer)       [(None, 1, 4096)]            0         []                            \n",
      "                                                                                                  \n",
      " OFFSOURCE (InputLayer)      [(None, 1, 32768)]           0         []                            \n",
      "                                                                                                  \n",
      " whiten (Whiten)             (None, 1, 2048)              0         ['ONSOURCE[0][0]',            \n",
      "                                                                     'OFFSOURCE[0][0]']           \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 2048, 1)              0         ['whiten[0][0]']              \n",
      "                                                                                                  \n",
      " Convolutional_1 (Conv1D)    (None, 2048, 8)              520       ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " ELU_1 (ELU)                 (None, 2048, 8)              0         ['Convolutional_1[0][0]']     \n",
      "                                                                                                  \n",
      " Pooling_1 (MaxPooling1D)    (None, 512, 8)               0         ['ELU_1[0][0]']               \n",
      "                                                                                                  \n",
      " Convolutional_2 (Conv1D)    (None, 512, 8)               2056      ['Pooling_1[0][0]']           \n",
      "                                                                                                  \n",
      " ELU_2 (ELU)                 (None, 512, 8)               0         ['Convolutional_2[0][0]']     \n",
      "                                                                                                  \n",
      " Convolutional_3 (Conv1D)    (None, 512, 16)              4112      ['ELU_2[0][0]']               \n",
      "                                                                                                  \n",
      " ELU_3 (ELU)                 (None, 512, 16)              0         ['Convolutional_3[0][0]']     \n",
      "                                                                                                  \n",
      " Pooling_3 (MaxPooling1D)    (None, 128, 16)              0         ['ELU_3[0][0]']               \n",
      "                                                                                                  \n",
      " Flatten (Flatten)           (None, 2048)                 0         ['Pooling_3[0][0]']           \n",
      "                                                                                                  \n",
      " Dense_1 (Dense)             (None, 64)                   131136    ['Flatten[0][0]']             \n",
      "                                                                                                  \n",
      " ELU_7 (ELU)                 (None, 64)                   0         ['Dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " Dropout_1 (Dropout)         (None, 64)                   0         ['ELU_7[0][0]']               \n",
      "                                                                                                  \n",
      " Dense_2 (Dense)             (None, 64)                   4160      ['Dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " ELU_8 (ELU)                 (None, 64)                   0         ['Dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " Dropout_2 (Dropout)         (None, 64)                   0         ['ELU_8[0][0]']               \n",
      "                                                                                                  \n",
      " INJECTION_MASKS (Dense)     (None, 1)                    65        ['Dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 142049 (554.88 KB)\n",
      "Trainable params: 142049 (554.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with env:\n",
    "    # This object will be used to obtain real interferometer data based on specified parameters.\n",
    "    ifo_data_obtainer : gf.IFODataObtainer = gf.IFODataObtainer(\n",
    "        observing_runs=gf.ObservingRun.O3, # Specify the observing run (e.g., O3).\n",
    "        data_quality=gf.DataQuality.BEST,  # Choose the quality of the data (e.g., BEST).\n",
    "        data_labels=[                      # Define the types of data to include.\n",
    "            gf.DataLabel.NOISE, \n",
    "            gf.DataLabel.GLITCHES\n",
    "        ],\n",
    "        segment_order=gf.SegmentOrder.RANDOM, # Order of segment retrieval (e.g., RANDOM).\n",
    "        force_acquisition=True,               # Force the acquisition of new data.\n",
    "        cache_segments=False                  # Choose not to cache the segments.\n",
    "    )\n",
    "\n",
    "    # Initialize the noise generator wrapper:\n",
    "    # This wrapper will use the ifo_data_obtainer to generate real noise based on the specified parameters.\n",
    "    noise: gf.NoiseObtainer = gf.NoiseObtainer(\n",
    "        ifo_data_obtainer=ifo_data_obtainer, # Use the previously set up IFODataObtainer object.\n",
    "        noise_type=gf.NoiseType.REAL,        # Specify the type of noise as REAL.\n",
    "        ifos=gf.IFO.L1                       # Specify the interferometer (e.g., LIGO Livingston L1).\n",
    "    )\n",
    "\n",
    "    scaling_method : gf.ScalingMethod = gf.ScalingMethod(\n",
    "        value=gf.Distribution(\n",
    "            min_=8.0,\n",
    "            max_=15.0,\n",
    "            type_=gf.DistributionType.UNIFORM\n",
    "        ),\n",
    "        type_=gf.ScalingTypes.SNR\n",
    "    )\n",
    "\n",
    "    # Define a uniform distribution for the mass of the first object in solar masses.\n",
    "    mass_1_distribution_msun : gf.Distribution = gf.Distribution(\n",
    "        min_=10.0, \n",
    "        max_=60.0, \n",
    "        type_=gf.DistributionType.UNIFORM\n",
    "    )\n",
    "\n",
    "    # Define a uniform distribution for the mass of the second object in solar masses.\n",
    "    mass_2_distribution_msun : gf.Distribution = gf.Distribution(\n",
    "        min_=10.0, \n",
    "        max_=60.0, \n",
    "        type_=gf.DistributionType.UNIFORM\n",
    "    )\n",
    "\n",
    "    # Define a uniform distribution for the inclination of the binary system in radians.\n",
    "    inclination_distribution_radians : gf.Distribution = gf.Distribution(\n",
    "        min_=0.0, \n",
    "        max_=np.pi, \n",
    "        type_=gf.DistributionType.UNIFORM\n",
    "    )\n",
    "\n",
    "    # Initialize a PhenomD waveform generator with the defined distributions.\n",
    "    # This generator will produce waveforms with randomly varied masses and inclination angles.\n",
    "    phenom_d_generator : gf.WaveformGenerator = gf.cuPhenomDGenerator(\n",
    "        mass_1_msun=mass_1_distribution_msun,\n",
    "        mass_2_msun=mass_2_distribution_msun,\n",
    "        inclination_radians=inclination_distribution_radians,\n",
    "        scaling_method=scaling_method,\n",
    "        injection_chance=0.5 # Set so half produced examples will not contain this signal\n",
    "    )\n",
    "    \n",
    "    training_dataset : tf.data.Dataset = gf.Dataset(       \n",
    "        noise_obtainer=noise,\n",
    "        waveform_generators=phenom_d_generator,\n",
    "        input_variables=[\n",
    "            gf.ReturnVariables.ONSOURCE, \n",
    "            gf.ReturnVariables.OFFSOURCE, \n",
    "        ],\n",
    "        output_variables=[\n",
    "            gf.ReturnVariables.INJECTION_MASKS\n",
    "        ]\n",
    "    ).map(adjust_features)\n",
    "\n",
    "    validation_dataset : tf.data.Dataset = gf.Dataset(       \n",
    "        noise_obtainer=noise,\n",
    "        waveform_generators=phenom_d_generator,\n",
    "        seed=1001, # Implement different seed to generate different waveforms,\n",
    "        group=\"validate\", # Ensure noise is pulled from those marked for validation.\n",
    "        input_variables=[\n",
    "            gf.ReturnVariables.ONSOURCE, \n",
    "            gf.ReturnVariables.OFFSOURCE, \n",
    "        ],\n",
    "        output_variables=[\n",
    "            gf.ReturnVariables.INJECTION_MASKS\n",
    "        ]\n",
    "    ).map(adjust_features)\n",
    "\n",
    "    testing_dataset : tf.data.Dataset = gf.Dataset(       \n",
    "        noise_obtainer=noise,\n",
    "        waveform_generators=phenom_d_generator,\n",
    "        seed=1002, # Implement different seed to generate different waveforms,\n",
    "        group=\"test\", # Ensure noise is pulled from those marked for validation.\n",
    "        input_variables=[\n",
    "            gf.ReturnVariables.ONSOURCE, \n",
    "            gf.ReturnVariables.OFFSOURCE, \n",
    "        ],\n",
    "        output_variables=[\n",
    "            gf.ReturnVariables.INJECTION_MASKS\n",
    "        ]\n",
    "    ).map(adjust_features)\n",
    "    \n",
    "    for input_example, _ in training_dataset.take(1):\n",
    "        input_shape_onsource = input_example[\"ONSOURCE\"].shape[1:]  # Exclude batch dimension    \n",
    "        input_shape_offsource = input_example[\"OFFSOURCE\"].shape[1:] \n",
    "\n",
    "    model = create_gabbard(input_shape_onsource, input_shape_offsource)\n",
    "\n",
    "    # Now you can print the model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Model compilation\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',  # Or any other loss function appropriate for your task\n",
    "        metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell failed it is most likely because you attempted to run it twice within the same kernel session. The kernal must be restarted in order to generate a fresh TensorFlow stratergy and recompile the model.\n",
    "\n",
    "Finally, we can train the model with our generated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael.norman/miniconda3/envs/gravyflow/lib/python3.10/site-packages/igwn_auth_utils/x509.py:98: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to not_valid_after_utc.\n",
      "  return (cert.not_valid_after - datetime.utcnow()).total_seconds()\n",
      "2024-03-04 06:53:54.258151: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9946"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael.norman/miniconda3/envs/gravyflow/lib/python3.10/site-packages/igwn_auth_utils/x509.py:98: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to not_valid_after_utc.\n",
      "  return (cert.not_valid_after - datetime.utcnow()).total_seconds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 85s 24ms/step - loss: 0.0196 - accuracy: 0.9946 - val_loss: 5.4360 - val_accuracy: 0.6005\n",
      "Epoch 2/10\n",
      "3123/3125 [============================>.] - ETA: 0s - loss: 0.0268 - accuracy: 0.9937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael.norman/miniconda3/envs/gravyflow/lib/python3.10/site-packages/igwn_auth_utils/x509.py:98: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to not_valid_after_utc.\n",
      "  return (cert.not_valid_after - datetime.utcnow()).total_seconds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 66s 21ms/step - loss: 0.0268 - accuracy: 0.9937 - val_loss: 2.0139 - val_accuracy: 0.7336\n",
      "Epoch 3/10\n",
      "3123/3125 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9943"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael.norman/miniconda3/envs/gravyflow/lib/python3.10/site-packages/igwn_auth_utils/x509.py:98: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to not_valid_after_utc.\n",
      "  return (cert.not_valid_after - datetime.utcnow()).total_seconds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 67s 21ms/step - loss: 0.0275 - accuracy: 0.9943 - val_loss: 6.0460 - val_accuracy: 0.6392\n",
      "Epoch 4/10\n",
      "3121/3125 [============================>.] - ETA: 0s - loss: 0.0405 - accuracy: 0.9926"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael.norman/miniconda3/envs/gravyflow/lib/python3.10/site-packages/igwn_auth_utils/x509.py:98: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to not_valid_after_utc.\n",
      "  return (cert.not_valid_after - datetime.utcnow()).total_seconds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 72s 23ms/step - loss: 0.0405 - accuracy: 0.9926 - val_loss: 5.9996 - val_accuracy: 0.6115\n",
      "Epoch 5/10\n",
      " 396/3125 [==>...........................] - ETA: 20s - loss: 0.0561 - accuracy: 0.9846"
     ]
    }
   ],
   "source": [
    "examples_per_epoch : int = int(1E5)\n",
    "num_validation_examples : int = int(1E5)\n",
    "num_testing_examples : int = int(1E5)\n",
    "with env: \n",
    "    history = model.fit(\n",
    "        training_dataset,\n",
    "        epochs=10,  # Number of epochs to train for\n",
    "        steps_per_epoch=examples_per_epoch // gf.Defaults.num_examples_per_batch,\n",
    "        validation_data=validation_dataset,\n",
    "        validation_steps=num_validation_examples // gf.Defaults.num_examples_per_batch #Ensure this is set as dataset is uncapped\n",
    "    )\n",
    "\n",
    "    model.evaluate(\n",
    "        testing_dataset, \n",
    "        steps=num_testing_examples // gf.Defaults.num_examples_per_batch\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('gravyflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0be68b4d857a3b55dcb84670c9ea8054a433650dc9da871ef592f2f480116ffb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
